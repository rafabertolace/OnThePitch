{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1816ff7",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4cf7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only Rafa\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06e1f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn\n",
    "import shap\n",
    "from sklearn.metrics import r2_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b295e690",
   "metadata": {},
   "source": [
    "# Loading the Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4f75c",
   "metadata": {},
   "source": [
    "## Merging the Seasons csv files (2019-2020 untill 2021-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39b6d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spain_2019_2020_1.csv', 'italy_2020_2021_2.csv', 'italy_2021_2022_1.csv', 'italy_2021_2022_2.csv', 'spain_2020_2021_1.csv', 'spain_2020_2021_2.csv', 'scotland_2020_2021_2.csv', 'Turkey_2021_2022_1.csv', 'scotland_2020_2021_3.csv', 'scotland_2019_2020_1.csv', 'france_2021_2022_1.csv', 'germany_2021_2022_1.csv', 'belgium_2020_2021_1.csv', 'scotland_2021_2022_4.csv', 'scotland_2020_2021_1.csv', 'france_2019_2020_1.csv', 'germany_2021_2022_2.csv', 'germany_2019_2020_1.csv', 'england_2020_2021_3.csv', 'scotland_2019_2020_2.csv', 'italy_2019_2020_1.csv', 'scotland_2020_2021_4.csv', 'portugal_2021_2022_1.csv', 'france_2020_2021_2.csv', 'scotland_2021_2022_3.csv', 'england_2021_2022_3.csv', 'portugal_2019_2020_1.csv', 'Greece_2021_2022_1.csv', 'england_2020_2021_4.csv', 'england_2021_2022_4.csv', 'france_2020_2021_1.csv', 'germany_2020_2021_1.csv', 'scotland_2019_2020_4.csv', 'spain_2019_20220_2.csv', 'germany_2019_2020_2.csv', 'england_2019_2020_4.csv', 'scotland_2021_2022_1.csv', 'england_2020_2021_2.csv', 'france_2019_2020_2.csv', 'Turkey_2019_2020_1.csv', 'england_2019_2020_2.csv', 'Eredivisie_2021_2022_1.csv', 'Turkey_2020_2021_1.csv', 'spain_2021_2022_1.csv', 'france_2021_2022_2.csv', 'belgium_2021_2022_1.csv', 'Greece_2019_2020_1.csv', 'belgium_2019_2020_1.csv', 'england_2019_2020_1.csv', 'Greece_2020_2021_1.csv', 'spain_2021_2022_2.csv', 'italy_2019_2020_2.csv', 'portugal_2020_2021_1.csv', 'Eredivisie_2020_2021_1.csv', 'italy_2020_2021_1.csv', 'Eredivisie_2019_2020_1.csv', 'england_2021_2022_2.csv', 'england_2020_2021_1.csv', 'england_2019_2020_3.csv', 'scotland_2021_2022_2.csv', 'scotland_2019_2020_3.csv', 'england_2021_2022_1.csv', 'germany_2020_2021_2.csv']\n"
     ]
    }
   ],
   "source": [
    "# Customise based on your path and folder organisation\n",
    "print(os.listdir('../raw_data/All4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2208c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the CSVs\n",
    "\n",
    "files = [file for file in os.listdir('../raw_data/All4') if file.endswith('.csv')]\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv('../raw_data/All4/' + file)\n",
    "    df['country']=str(file)[0:2]\n",
    "    df['country_division']=int(str(file)[-5:-4])\n",
    "    data = pd.concat([data, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8faf8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(league1, wall=False):\n",
    "    if wall:\n",
    "        data = pd.DataFrame()\n",
    "        leagues = listdir(f'./../raw_data/')\n",
    "        data = pd.DataFrame()\n",
    "        for league in leagues:\n",
    "            files = listdir(f'./../raw_data/{league}')\n",
    "            for file in files:\n",
    "                df = pd.read_csv((f'./../raw_data/{league}/'+file))\n",
    "                df['country']=str(file)[0:2]\n",
    "                data = pd.concat([data, df])\n",
    "        return data\n",
    "    else:\n",
    "        files = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "        data = pd.DataFrame()\n",
    "        for file in files:\n",
    "            df = pd.read_csv(f'./../raw_data/{league1}/'+file)\n",
    "            data = pd.concat([data, df])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8ad8d",
   "metadata": {},
   "source": [
    "# Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acad4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the function\n",
    "\n",
    "def feature_engineering(data, b=20, binned=False):\n",
    "    '''\n",
    "    This function creates all the columns that will be needed to create the analysis \n",
    "    and return the dataframe with all this changes\n",
    "    \n",
    "    b is the number of bins that we want to work with. Our start value for b will be 20.\n",
    "        '''\n",
    "    #------------------------Number of Goals, Over and Under -----------------------------------\n",
    "    \n",
    "    # total number of goals = goals from the home team + goals from visiting team\n",
    "    data['nb_goals']=data['FTHG']+data['FTAG']\n",
    "\n",
    "    # boolean: true or false regarding whether they were more than 2.5 goals\n",
    "    data['over_2.5_goals']=data['nb_goals']>2.5\n",
    "\n",
    "    # boolean: true or false regarding whether they were less than 2.5 goals\n",
    "    data['under_2.5_goals']=data['nb_goals']<2.5\n",
    "    \n",
    "    #-----------------------------Payout Opening ----------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average OPENING odds\n",
    "    data['payout_avg_under_2.5'] = data['under_2.5_goals']*data['Avg<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average OPENING odds\n",
    "    data['payout_avg_over_2.5'] = data['over_2.5_goals']*data['Avg>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE specifically\n",
    "    data['payout_under_2.5_pinacle'] = data['under_2.5_goals']*data['P<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE specifically\n",
    "    data['payout_over_2.5_pinacle'] = data['over_2.5_goals']*data['P>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 specifically\n",
    "    data['payout_under_2.5_365'] = data['under_2.5_goals']*data['B365<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 specifically\n",
    "    data['payout_over_2.5_365'] = data['over_2.5_goals']*data['B365>2.5']\n",
    "    \n",
    "    #------------------------------Payout Closing --------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_under_closing_2.5'] = data['under_2.5_goals']*data['AvgC<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_over_closing_2.5'] = data['over_2.5_goals']*data['AvgC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE closing ddds specifically\n",
    "    data['payout_under_2.5_pinacle_closing'] = data['under_2.5_goals']*data['PC<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE closing odds specifically\n",
    "    data['payout_over_2.5_pinacle_closing'] = data['over_2.5_goals']*data['PC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 closing odds specifically\n",
    "    data['payout_under_2.5_365_closing'] = data['under_2.5_goals']*data['B365C<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 closing odds specifically\n",
    "    data['payout_over_2.5_365_closing'] = data['over_2.5_goals']*data['B365C>2.5']\n",
    "    \n",
    "    #-------------------------- Implied Probability Opening ----------------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability <2.5 avg']=1/data['Avg<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability >2.5 avg']=1/data['Avg>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE\n",
    "    data['Implied Probability <2.5 pinacle']=1/data['P<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE\n",
    "    data['Implied Probability >2.5 pinacle']=1/data['P>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365\n",
    "    data['Implied Probability <2.5 365']=1/data['B365<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365\n",
    "    data['Implied Probability >2.5 365']=1/data['B365>2.5']*100\n",
    "    \n",
    "    #------------------------- Implied Probability Closing -----------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability <2.5 avg closing']=1/data['AvgC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability >2.5 avg closing']=1/data['AvgC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability <2.5 pinacle closing']=1/data['PC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability >2.5 pinacle closing']=1/data['PC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability <2.5 365 closing']=1/data['B365C<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability >2.5 365 closing']=1/data['B365C>2.5']*100\n",
    "    \n",
    "    #---------------------------- Binning IP Opening -------------------------------------\n",
    "\n",
    "    b=b\n",
    "    bins = np.arange(0, 101, int(100/b))\n",
    "    bins = bins.tolist()\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned <2.5 avg'] = pd.cut(data['Implied Probability <2.5 avg'], bins)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned >2.5 avg'] = pd.cut(data['Implied Probability >2.5 avg'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned <2.5 pinacle'] = pd.cut(data['Implied Probability <2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned >2.5 pinacle'] = pd.cut(data['Implied Probability >2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned <2.5 365'] = pd.cut(data['Implied Probability <2.5 365'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned >2.5 365'] = pd.cut(data['Implied Probability >2.5 365'], bins)\n",
    "    \n",
    "    #----------------------------- Binning IP Closing ------------------------------------------------\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market closing odds\n",
    "    data['binned <2.5 avg closing'] = pd.cut(data['Implied Probability <2.5 avg closing'], bins)\n",
    "\n",
    "    #Binning OVER 2.5 Average Market closing odds\n",
    "    data['binned >2.5 avg closing'] = pd.cut(data['Implied Probability >2.5 avg closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle closing odds\n",
    "    data['binned <2.5 pinacle closing'] = pd.cut(data['Implied Probability <2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle CLOSING odds\n",
    "    data['binned >2.5 pinacle closing'] = pd.cut(data['Implied Probability >2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 CLOSING odds\n",
    "    data['binned <2.5 365 closing'] = pd.cut(data['Implied Probability <2.5 365 closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 CLOSING odds\n",
    "    data['binned >2.5 365 closing'] = pd.cut(data['Implied Probability >2.5 365 closing'], bins)\n",
    "    \n",
    "    #---------------------------- Binning Odds Opening ----------------------------------------------------\n",
    "    \n",
    "    bins2 = [1, 1.5, 2, 3, 10]\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg'] = pd.cut(data['Avg<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg'] = pd.cut(data['Avg>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle'] = pd.cut(data['P<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle'] = pd.cut(data['P>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365'] = pd.cut(data['B365<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365'] = pd.cut(data['B365>2.5'], bins2)\n",
    "    \n",
    "    #----------------------------- Binning Odds Closing ----------------------------------------------------------\n",
    "    \n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg closing'] = pd.cut(data['AvgC<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg closing'] = pd.cut(data['AvgC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle closing'] = pd.cut(data['PC<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle closing'] = pd.cut(data['PC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365 closing'] = pd.cut(data['B365C<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365 closing'] = pd.cut(data['B365C>2.5'], bins2)\n",
    "    \n",
    "    \n",
    "    #----------------------------- Other Features from D3 ------------------------------------------------------\n",
    "    \n",
    "    data['Pin_pays_better_under_boolean'] = data['PC<2.5'] > data['AvgC<2.5']\n",
    "    data['Pin_pays_better_under_difference'] = data['PC<2.5'] / data['AvgC<2.5']\n",
    "    data['%vig_p'] = (1 - (1 / (1/data['PC>2.5'] + 1/data['PC<2.5'])))*100\n",
    "    data['%vig_avg'] = (1 - (1 / (1/data['AvgC>2.5'] + 1/data['AvgC<2.5'])))*100\n",
    "    data['PC<2.5_P_boolean'] = data['PC<2.5'] < data['P<2.5']\n",
    "    data['PC<2.5_P_relative_diff'] = data['PC<2.5'] / data['P<2.5']\n",
    "    \n",
    "    #----------------------- Odds and probability of the home team scoring under 2.5 -------------------------------\n",
    "    \n",
    "\n",
    "    \n",
    "    #------------------------------------ Cleaning the data ---------------------------------------------------------\n",
    "    \n",
    "    #data = data.dropna(subset=['HomeTeam', 'AwayTeam'], how='any')\n",
    "    data = data[~data['HomeTeam'].isna()]\n",
    "    data = data[~data['AwayTeam'].isna()]\n",
    "    data = data[~data['PC>2.5'].isna()]\n",
    "    data.drop(columns=['Referee','Unnamed: 105'], inplace=True) #, 'Unnamed: 105' 'Referee', \n",
    "    #data.dropna()\n",
    "    \n",
    "     #-------------------------- OneHotEncoding the binned odds ------------------------------------------\n",
    "   \n",
    "    ohe = OneHotEncoder(sparse=False) \n",
    "    ohe.fit(data[['binned odds <2.5 pinacle closing']])\n",
    "    bins_encoded = ohe.transform(data[['binned odds <2.5 pinacle closing']])\n",
    "    data[\"1.0_to_1.5\"], data[\"1.5_to_2.0\"], data[\"2.0_to_3\"], data[\"3_to_10\"] = bins_encoded.T\n",
    "    data.drop(columns='binned odds <2.5 pinacle closing', inplace=True)\n",
    "    \n",
    "    #-------------------------- OneHotEncoding the binned odds ------------------------------------------\n",
    "   \n",
    "    ohe = OneHotEncoder(sparse=False) \n",
    "    ohe.fit(data[['binned odds <2.5 avg closing']])\n",
    "    bins_encoded = ohe.transform(data[['binned odds <2.5 avg closing']])\n",
    "    data[\"avg_1.0_to_1.5\"], data[\"avg_1.5_to_2.0\"], data[\"avg_2.0_to_3\"], data[\"avg_3_to_10\"] = bins_encoded.T\n",
    "    data.drop(columns='binned odds <2.5 avg closing', inplace=True)\n",
    "    \n",
    "    \n",
    "    #-------------------------- OneHotEncoding the binned countries ------------------------------------------\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False) \n",
    "    ohe.fit(data[['country']])\n",
    "    bins_encoded = ohe.transform(data[['country']])\n",
    "    data[\"country_1\"], data[\"country_2\"], data[\"country_3\"], data[\"country_4\"], data[\"country_5\"],data[\"country_6\"], data[\"country_7\"], data[\"country_8\"], data[\"country_9\"], data[\"country_10\"], data[\"country_11\"] = bins_encoded.T\n",
    "    data.drop(columns='country', inplace=True)\n",
    "\n",
    "    #-------------------------- OneHotEncoding the binned country divisions ------------------------------------------\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse=False) \n",
    "    ohe.fit(data[['country_division']])\n",
    "    bins_encoded = ohe.transform(data[['country_division']])\n",
    "    data[\"country_div_1\"], data[\"country_div_2\"], data[\"country_div_3\"], data[\"country_div_4\"] = bins_encoded.T\n",
    "    #data.drop(columns='country_division', inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c518df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the function and creating the dataset data\n",
    "\n",
    "data = feature_engineering(data, b=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24d725b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WE WILL NEED TO ADD ALL THOSE IN THE feature_engineering FUNCTION\n",
    "\n",
    "## Adding the Year Feature \n",
    "data_date = data['Date']\n",
    "data_time = data['Time']\n",
    "data_date_2 = pd.to_datetime(data_date, dayfirst = True)\n",
    "data_time_2 = pd.to_datetime(data_time, dayfirst = True)\n",
    "data['month'] = pd.DatetimeIndex(data_date_2).month\n",
    "data['month_after_July'] = data['month']>=7\n",
    "data['year'] = pd.DatetimeIndex(data_date_2).year\n",
    "data['year_2021_2022'] = data['year']>=2021\n",
    "data['year_2022'] = data['year']>=2022\n",
    "data['year_2020'] = data['year']==2020\n",
    "data['season_21_22'] = data_date_2>='2021-09-01'\n",
    "data['season_20_21'] = (data_date_2>='2020-09-01') & (data_date_2<'2021-09-01')\n",
    "data['season_training2'] = (data_date_2<'2020-09-01')\n",
    "\n",
    "data['hour'] = pd.DatetimeIndex(data_time_2).hour\n",
    "data['game_starts_after_4pm']=data['hour']>=16\n",
    "\n",
    "#Other features\n",
    "data['Pin_pays_better_under_boolean'] = data['PC<2.5'] > data['AvgC<2.5']\n",
    "data['Pin_pays_better_under_difference'] = data['PC<2.5'] / data['AvgC<2.5']\n",
    "data['%vig_p'] = (1 - (1 / (1/data['PC>2.5'] + 1/data['PC<2.5'])))*100\n",
    "data['%vig_p_bool'] = data['%vig_p']>3.3\n",
    "data['%vig_avg'] = (1 - (1 / (1/data['AvgC>2.5'] + 1/data['AvgC<2.5'])))*100\n",
    "data['%vig_avg_bool'] = data['%vig_avg']>5.4\n",
    "data['PC<2.5_P_boolean'] = data['PC<2.5'] < data['P<2.5']\n",
    "data['PC<2.5_P_relative_diff'] = data['PC<2.5'] / data['P<2.5']\n",
    "data['MaxC>2.5_AvgC_relative_diff'] = data['MaxC>2.5']/data['AvgC>2.5']\n",
    "data['Market_consensus'] = data['MaxC>2.5_AvgC_relative_diff']<1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ba33728",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month_number']=data['month']\n",
    "data['month_number_ratio']=data['month']/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4accb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['year_number'] = data['year'].map({2019:1, 2020:2, 2021:3, 2022:4})\n",
    "data['year_number_ratio'] = data['year'].map({2019:1, 2020:2, 2021:3, 2022:4})/4\n",
    "data['year_number_month_ratio'] = data['year_number']+data['month_number_ratio']\n",
    "data['year_number_month_decimal'] = data['year_number']+data['month_number']/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9523dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False) \n",
    "ohe.fit(data[['year_number']])\n",
    "bins_encoded = ohe.transform(data[['year_number']])\n",
    "data[\"year_1\"], data[\"year_2\"], data[\"year_3\"], data[\"year_4\"] = bins_encoded.T\n",
    "   # data.drop(columns='country_division', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6794277",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False) \n",
    "ohe.fit(data[['month_number']])\n",
    "bins_encoded = ohe.transform(data[['month_number']])\n",
    "data[\"month_1\"], data[\"month_2\"], data[\"month_3\"], data[\"month_4\"], data[\"month_5\"], data[\"month_6\"], data[\"month_7\"], data[\"month_8\"], data[\"month_9\"], data[\"month_10\"], data[\"month_11\"], data[\"month_12\"] = bins_encoded.T\n",
    "   # data.drop(columns='country_division', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94006a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False) \n",
    "ohe.fit(data[['game_starts_after_4pm']])\n",
    "bins_encoded = ohe.transform(data[['game_starts_after_4pm']])\n",
    "data[\"game_starts_after_4pm_1\"], data[\"game_starts_after_4pm_2\"] = bins_encoded.T\n",
    "   # data.drop(columns='country_division', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83ce4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False) \n",
    "ohe.fit(data[['game_starts_after_4pm']])\n",
    "bins_encoded = ohe.transform(data[['game_starts_after_4pm']])\n",
    "data[\"game_starts_after_4pm_1\"], data[\"game_starts_after_4pm_2\"] = bins_encoded.T\n",
    "   # data.drop(columns='country_division', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8eb704b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hour_number'] = data['hour'].map({10:1, 11:2, 12:3, 13:4, 14:5, 15:6, 16:7, 17:8, 18:9, 19:10, 20:11, 21:12})\n",
    "data['hour_number_2'] = data['hour'].map({10:1, 11:1, 12:1, 13:2, 14:2, 15:2, 16:3, 17:3, 18:3, 19:4, 20:4, 21:4})\n",
    "data['hour_number_2_ratio'] = data['hour'].map({10:1, 11:1, 12:1, 13:2, 14:2, 15:2, 16:3, 17:3, 18:3, 19:4, 20:4, 21:4})/4\n",
    "data['hour_number_ratio'] = data['hour'].map({10:1, 11:2, 12:3, 13:4, 14:5, 15:6, 16:7, 17:8, 18:9, 19:10, 20:11, 21:12})/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "308990e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False) \n",
    "ohe.fit(data[['hour_number_2']])\n",
    "bins_encoded = ohe.transform(data[['hour_number_2']])\n",
    "data[\"hour_group_1\"], data[\"hour_group_2\"], data[\"hour_group_3\"], data[\"hour_group_4\"] = bins_encoded.T\n",
    "   # data.drop(columns='country_division', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1b178ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PC<2.5']\n",
    "\n",
    "data.rename(columns = {'PC<2.5':'PC_under_2.5'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0a0f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     ohe = OneHotEncoder(sparse=False) \n",
    "#     ohe.fit(data[[\"binned_odds\"]])\n",
    "#     bins_encoded = ohe.transform(data[[\"binned_odds\"]])\n",
    "#     data[\"bin_odds_1\"], data[\"bin_odds_2\"],data[\"bin_odds_3\"], data[\"bin_odds_4\"] = bins_encoded.T\n",
    "   # data.drop(columns='country_division', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0df27",
   "metadata": {},
   "source": [
    "# Running the XGB model for under 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "899a0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model dataset with relevant variables\n",
    "data_linear_booleans_lean_P_under = data[['year_number','year_number_ratio','year_number_month_ratio','year_number_month_decimal','month_number','month_number_ratio','country_div_1','country_div_2','country_div_3','country_div_4','month_after_July','season_21_22','year_2020','game_starts_after_4pm','Market_consensus','%vig_p_bool','%vig_avg_bool','avg_1.0_to_1.5','avg_1.5_to_2.0','avg_2.0_to_3','avg_3_to_10','payout_under_2.5_pinacle_closing','year_1','year_2','year_3','year_4','month_1','month_2','month_3','month_4','month_5','month_6','month_7','month_8','month_9','month_10','month_11','month_12','hour_number','hour_number_ratio','hour_number_2','hour_number_2_ratio',\"game_starts_after_4pm_1\",\"game_starts_after_4pm_2\",'hour_group_1','hour_group_2','hour_group_3','hour_group_4','country_division','season_training2','season_20_21','payout_avg_under_closing_2.5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0bb8a",
   "metadata": {},
   "source": [
    "## Defining the function testing_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "220dd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back up\n",
    "def testing_models(iterations):   \n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i < iterations:\n",
    "        # Split into Train/Test\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3) # Split into Train/Test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_test_0, y_test_0, test_size=test_size_0) # Split into Train/Test\n",
    "\n",
    "            #------------------------Run the Models -----------------------------------\n",
    "\n",
    "        #Initiate XGBoost\n",
    "        m = 0\n",
    "        m = xgb.XGBRegressor()\n",
    "        \n",
    "        #Rename columns for XGBoost to run\n",
    "        ##X_test.rename(columns = {'PC<2.5_P_boolean':'PC_under_2.5_P_boolean'}, inplace = True)\n",
    "        ##X_train.rename(columns = {'PC<2.5_P_boolean':'PC_under_2.5_P_boolean'}, inplace = True)\n",
    "        \n",
    "        #Fit XGBoost\n",
    "        m.fit(X_train_0,y_train_0) \n",
    "        \n",
    "        #Make and store the predictions XGBoost\n",
    "        y_pred_xgb = m.predict(X_test)\n",
    "        y_pred_xgb = pd.DataFrame(y_pred_xgb)\n",
    "\n",
    "            #------------------------Creating the bins for the predictions for both models -----------------------------------\n",
    "        #Function to replace bin with default value when it is null\n",
    "        def ifnull(var, val):\n",
    "          if var > 0:\n",
    "            return var\n",
    "          return val\n",
    "\n",
    "        #XGB bins\n",
    "        y_pred_xgb_under_0_median = y_pred_xgb[y_pred_xgb<0].median()\n",
    "        y_pred_xgb_under_0_min = y_pred_xgb[y_pred_xgb<0].min()\n",
    "        y_pred_xgb_over_0_median = ifnull(y_pred_xgb[0][y_pred_xgb[0]>0].median(),0.05)\n",
    "\n",
    "            #------------------------Betting decisions for both models -----------------------------------\n",
    "\n",
    "        #XGB\n",
    "        bins3_xgb = [y_pred_xgb_under_0_min[0]-0.0000002, y_pred_xgb_under_0_median[0]-0.0000001, 0, y_pred_xgb_over_0_median+0.0000001, 1] #int(y_pred_xgb_over_0_median[0])\n",
    "        y_xgb_df = pd.DataFrame(y_test)\n",
    "        y_pred_xgb_df = pd.DataFrame(y_pred_xgb)\n",
    "        y_pred_xgb_df[\"binned_pred\"] = pd.cut(y_pred_xgb_df[0], bins3_xgb)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_1\"] = y_pred_xgb_df[0]<y_pred_xgb_under_0_median[0]-0.0000001\n",
    "        y_pred_xgb_df[\"binned_pred_bin_2\"] = (y_pred_xgb_df[0]>y_pred_xgb_under_0_median[0]-0.0000001) & (y_pred_xgb_df[0]<0)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_3\"] = (y_pred_xgb_df[0]>0) & (y_pred_xgb_df[0]<y_pred_xgb_over_0_median+0.0000001)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_4\"] = (y_pred_xgb_df[0]>y_pred_xgb_over_0_median+0.0000001)\n",
    "        y_pred_xgb_df[\"bin_number\"] = y_pred_xgb_df[\"binned_pred_bin_1\"]*1 + y_pred_xgb_df[\"binned_pred_bin_2\"]*2 + y_pred_xgb_df[\"binned_pred_bin_3\"]*3 + y_pred_xgb_df[\"binned_pred_bin_4\"]*4     \n",
    "        ind = np.arange(0, len(y_pred_xgb_df))\n",
    "        ind = ind.tolist()\n",
    "        y_xgb_df['ind'] = ind\n",
    "        y_pred_xgb_df['ind'] = ind\n",
    "        y_final_xgb = y_xgb_df.merge(y_pred_xgb_df, on=\"ind\")#, on = \"axis\")#, how = \"inner\")\n",
    "        \n",
    "        #Defining the variable bet_opp based on the y_pred of the XGB\n",
    "        y_final_xgb['bet_opp']=y_final_xgb[0]>0\n",
    "\n",
    "            #------------------------Getting the results based on predictions -----------------------------------    \n",
    "\n",
    "        #XGB\n",
    "        #xgb_count = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].count()\n",
    "        #xgb_mean = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].mean()\n",
    "        #xgb_results = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].agg([\"mean\", \"count\"])\n",
    "        xgb_results_False = y_final_xgb[y_final_xgb['bet_opp']==False]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_True = y_final_xgb[y_final_xgb['bet_opp']==True]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_1 = y_final_xgb[y_final_xgb['bin_number']==1]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_2 = y_final_xgb[y_final_xgb['bin_number']==2]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_3 = y_final_xgb[y_final_xgb['bin_number']==3]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_4 = y_final_xgb[y_final_xgb['bin_number']==4]['payout_avg_under_closing_2.5'].mean()\n",
    "        \n",
    "        results_xgb_false.append(xgb_results_False)\n",
    "        results_xgb_true.append(xgb_results_True)\n",
    "        results_xgb_1.append(xgb_results_bin_1)\n",
    "        results_xgb_2.append(xgb_results_bin_2)\n",
    "        results_xgb_3.append(xgb_results_bin_3)\n",
    "        results_xgb_4.append(xgb_results_bin_4)\n",
    "        \n",
    "        return y_test, y_train_0, y_pred_xgb, y_final_xgb \n",
    "        \n",
    "        i=i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60320b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_lr_true = []\n",
    "# results_lr_false = []\n",
    "\n",
    "def testing_models_classifier(iterations):   \n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i < iterations:\n",
    "        # Split into Train/Test\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3) # Split into Train/Test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_test_0, y_test_0_classifier, test_size=test_size_0) # Split into Train/Test\n",
    "\n",
    "            #------------------------Run the Models -----------------------------------\n",
    "\n",
    "        #Initiate XGBoost\n",
    "        m = 0\n",
    "        m = xgb.XGBClassifier()\n",
    "        \n",
    "        #Fit XGBoost\n",
    "        m.fit(X_train_0,y_train_0_classifier) \n",
    "        \n",
    "        #Make and store the predictions XGBoost\n",
    "        y_pred_test = m.predict(X_test)\n",
    "        y_pred_test_df = pd.DataFrame(y_pred_test,columns=['prediction'])\n",
    "        y_pred_train = m.predict(X_train_0)\n",
    "        y_pred_train_df = pd.DataFrame(y_pred_train)\n",
    "        y_test_df = pd.DataFrame(y_test).reset_index()\n",
    "        y_final_df = y_pred_test_df.copy()\n",
    "        y_final_df['actual'] = y_test_df['payout_avg_under_closing_2.5']\n",
    "\n",
    "        \n",
    "        # accuracy\n",
    "        accuracy_test = accuracy_score(y_test,y_pred_test)\n",
    "        accuracy_train = accuracy_score(y_train_0_classifier,y_pred_train)\n",
    "        \n",
    "        # precision\n",
    "        precision_test = precision_score(y_test,y_pred_test)\n",
    "        precision_train = precision_score(y_train_0_classifier,y_pred_train)\n",
    "    \n",
    "  #------------------------Creating the bins for the predictions for both models -----------------------------------\n",
    "        #Function to replace bin with default value when it is null\n",
    "        def ifnull(var, val):\n",
    "          if var > 0:\n",
    "            return var\n",
    "          return val\n",
    "        \n",
    "        #XGB\n",
    "        #ind = np.arange(0, len(y_pred_test_df))\n",
    "        #ind = ind.tolist()\n",
    "        #y_xgb_df['ind'] = ind\n",
    "        #y_pred_test_df['ind'] = ind\n",
    "        \n",
    "        #Defining the variable bet_opp based on the y_pred of the XGB\n",
    "\n",
    "        y_final_df['payout_avg_under_closing_2.5'] = y_test_0.loc[y_test.index].reset_index()['payout_avg_under_closing_2.5']\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        #y_final_df['payout_avg_under_closing_2.5'] = 0\n",
    "\n",
    "        result_False = y_final_df[y_final_df['prediction']==False]['payout_avg_under_closing_2.5'].mean()\n",
    "        result_True = y_final_df[y_final_df['prediction']==True]['payout_avg_under_closing_2.5'].mean()\n",
    "        \n",
    "        results_False.append(result_False)\n",
    "        results_True.append(result_True)\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "    return accuracy_test, accuracy_train, precision_train, precision_test, y_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27bb9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test, accuracy_train, precision_train, precision_test, y_final_df = testing_models_classifier(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "480f4ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5593734209196564,\n",
       " 0.5889999301139143,\n",
       " 0.5811435660313522,\n",
       " 0.5505816894284269)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test, accuracy_train, precision_train, precision_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab21f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_lr_true = []\n",
    "# results_lr_false = []\n",
    "\n",
    "def testing_models2(iterations):   \n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i < iterations:\n",
    "        # Split into Train/Test\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3) # Split into Train/Test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_test_0, y_test_0, test_size=test_size_0) # Split into Train/Test\n",
    "\n",
    "            #------------------------Run the Models -----------------------------------\n",
    "\n",
    "        #Initiate XGBoost\n",
    "        m = 0\n",
    "        m = xgb.XGBRegressor()\n",
    "        \n",
    "        #Rename columns for XGBoost to run\n",
    "        ##X_test.rename(columns = {'PC<2.5_P_boolean':'PC_under_2.5_P_boolean'}, inplace = True)\n",
    "        ##X_train.rename(columns = {'PC<2.5_P_boolean':'PC_under_2.5_P_boolean'}, inplace = True)\n",
    "        \n",
    "        #Fit XGBoost\n",
    "        m.fit(X_train_0,y_train_0) \n",
    "\n",
    "        #Make and store the predictions XGBoost\n",
    "        y_pred_xgb_vector = m.predict(X_test)\n",
    "        y_pred_xgb = pd.DataFrame(y_pred_xgb_vector)\n",
    "        y_pred_xgb_train_vector = m.predict(X_train_0)\n",
    "        y_pred_xgb_train = pd.DataFrame(y_pred_xgb_train_vector)\n",
    "        r2_score_test = r2_score(y_test,y_pred_xgb_vector)\n",
    "        r2_score_train = r2_score(y_train_0,y_pred_xgb_train_vector)\n",
    "\n",
    "            #------------------------Creating the bins for the predictions for both models -----------------------------------\n",
    "        #Function to replace bin with default value when it is null\n",
    "        def ifnull(var, val):\n",
    "          if var > 0:\n",
    "            return var\n",
    "          return val\n",
    "\n",
    "        #XGB bins\n",
    "        y_pred_xgb_under_0_median = y_pred_xgb[y_pred_xgb<0].median()\n",
    "        y_pred_xgb_under_0_min = y_pred_xgb[y_pred_xgb<0].min()\n",
    "        y_pred_xgb_over_0_median = ifnull(y_pred_xgb[0][y_pred_xgb[0]>0].median(),0.05)\n",
    "\n",
    "            #------------------------Betting decisions for both models -----------------------------------\n",
    "\n",
    "        #XGB\n",
    "        bins3_xgb = [y_pred_xgb_under_0_min[0]-0.0000002, y_pred_xgb_under_0_median[0]-0.0000001, 0, y_pred_xgb_over_0_median+0.0000001, 1] #int(y_pred_xgb_over_0_median[0])\n",
    "        y_xgb_df = pd.DataFrame(y_test)\n",
    "        y_pred_xgb_df = pd.DataFrame(y_pred_xgb)\n",
    "        y_pred_xgb_df[\"binned_pred\"] = pd.cut(y_pred_xgb_df[0], bins3_xgb)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_1\"] = y_pred_xgb_df[0]<y_pred_xgb_under_0_median[0]-0.0000001\n",
    "        y_pred_xgb_df[\"binned_pred_bin_2\"] = (y_pred_xgb_df[0]>y_pred_xgb_under_0_median[0]-0.0000001) & (y_pred_xgb_df[0]<0)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_3\"] = (y_pred_xgb_df[0]>0) & (y_pred_xgb_df[0]<y_pred_xgb_over_0_median+0.0000001)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_4\"] = (y_pred_xgb_df[0]>y_pred_xgb_over_0_median+0.0000001)\n",
    "        y_pred_xgb_df[\"bin_number\"] = y_pred_xgb_df[\"binned_pred_bin_1\"]*1 + y_pred_xgb_df[\"binned_pred_bin_2\"]*2 + y_pred_xgb_df[\"binned_pred_bin_3\"]*3 + y_pred_xgb_df[\"binned_pred_bin_4\"]*4     \n",
    "        ind = np.arange(0, len(y_pred_xgb_df))\n",
    "        ind = ind.tolist()\n",
    "        y_xgb_df['ind'] = ind\n",
    "        y_pred_xgb_df['ind'] = ind\n",
    "        y_final_xgb = y_xgb_df.merge(y_pred_xgb_df, on=\"ind\")#, on = \"axis\")#, how = \"inner\")\n",
    "        \n",
    "        #Defining the variable bet_opp based on the y_pred of the XGB\n",
    "        y_final_xgb['bet_opp']=y_final_xgb[0]>0\n",
    "\n",
    "            #------------------------Getting the results based on predictions -----------------------------------     \n",
    "\n",
    "        #XGB\n",
    "        #xgb_count = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].count()\n",
    "        #xgb_mean = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].mean()\n",
    "        #xgb_results = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].agg([\"mean\", \"count\"])\n",
    "        xgb_results_False = y_final_xgb[y_final_xgb['bet_opp']==False]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_True = y_final_xgb[y_final_xgb['bet_opp']==True]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_1 = y_final_xgb[y_final_xgb['bin_number']==1]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_2 = y_final_xgb[y_final_xgb['bin_number']==2]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_3 = y_final_xgb[y_final_xgb['bin_number']==3]['payout_avg_under_closing_2.5'].mean()\n",
    "        xgb_results_bin_4 = y_final_xgb[y_final_xgb['bin_number']==4]['payout_avg_under_closing_2.5'].mean()\n",
    "        \n",
    "        results_xgb_false.append(xgb_results_False)\n",
    "        results_xgb_true.append(xgb_results_True)\n",
    "        results_xgb_1.append(xgb_results_bin_1)\n",
    "        results_xgb_2.append(xgb_results_bin_2)\n",
    "        results_xgb_3.append(xgb_results_bin_3)\n",
    "        results_xgb_4.append(xgb_results_bin_4)\n",
    "        \n",
    "        return y_test, y_train_0, y_pred_xgb, y_final_xgb, r2_score_test, r2_score_train\n",
    "        \n",
    "        i=i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dac4f9",
   "metadata": {},
   "source": [
    "## Running the function testing_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b44a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the indices\n",
    "data_linear_booleans_lean_P_under.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Select here the variables you want to include in the testing\n",
    "#'hour_number','hour_number_ratio',\"hour_10\",\"hour_11\",\"hour_12\",\"hour_13\",\"hour_14\",\"hour_15\",\"hour_16\",\"hour_17\",\"hour_18\",\"hour_19\",\"hour_20\",\"hour_21\",'hour_number','hour_number_ratio','hour_number_2_ratio'\n",
    "X_train_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==False][['country_div_1','country_div_2','country_div_3','country_div_4','%vig_avg_bool','Market_consensus','year_number',\"game_starts_after_4pm_2\",\"game_starts_after_4pm_1\",'avg_1.0_to_1.5','avg_1.5_to_2.0','avg_2.0_to_3','avg_3_to_10']]#,'year_number_ratio','month_number_ratio']]\n",
    "X_test_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==True][['country_div_1','country_div_2','country_div_3','country_div_4','%vig_avg_bool','Market_consensus','year_number',\"game_starts_after_4pm_2\",\"game_starts_after_4pm_1\",'avg_1.0_to_1.5','avg_1.5_to_2.0','avg_2.0_to_3','avg_3_to_10']]#,'year_number_ratio','month_number_ratio']]\n",
    "y_train_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==False]['payout_avg_under_closing_2.5']-1\n",
    "y_test_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==True]['payout_avg_under_closing_2.5']-1\n",
    "\n",
    "#creating the classifier model\n",
    "y_test_0_classifier = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==True]['payout_avg_under_closing_2.5']!=0\n",
    "y_train_0_classifier = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==False]['payout_avg_under_closing_2.5']!=0\n",
    "\n",
    "#div_1',country_div_1','country_div_2','country_div_3','country_div_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a8a24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function testing_models\n",
    "results_xgb_true = [] \n",
    "results_xgb_false = []\n",
    "results_xgb_1=[]\n",
    "results_xgb_2=[]\n",
    "results_xgb_3=[]\n",
    "results_xgb_4=[]\n",
    "results_False=[]\n",
    "results_True=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4ffb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size_0=0.90\n",
    "# testing_models_classifier(100)\n",
    "accuracy_test, accuracy_train, precision_train, precision_test, y_final_df = testing_models_classifier(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "984bbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_model_classifier_results(n=10):\n",
    "    \n",
    "    #empty_lists\n",
    "    results_False=[]\n",
    "    results_True=[]\n",
    "    \n",
    "    #empty_lists\n",
    "    accuracy_test, accuracy_train, precision_train, precision_test, y_final_df = testing_models_classifier(n)\n",
    "    median_results_bad_bets = np.median(np.array(results_False))\n",
    "    median_results_good_bets = np.median(np.array(results_True))\n",
    "    \n",
    "    #Plotting the results of the XGB\n",
    "    sns.histplot(results_True,kde=True,bins=20,color='green')\n",
    "    sns.histplot(results_False,kde=True,bins=20,color='red')\n",
    "    \n",
    "    return median_results_bad_bets, median_results_good_bets, accuracy_test, accuracy_train, precision_train, precision_test, y_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8998cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean of empty slice.\n",
      "invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Results of the X tests with XGB (that you ran with the function testing_models)\n",
    "median_results_bad_bets = np.median(np.array(results_xgb_false))\n",
    "median_results_good_bets = np.median(np.array(results_xgb_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of the X tests with XGB (that you ran with the function testing_models)\n",
    "median_results_bad_bets = np.median(np.array(results_False))\n",
    "median_results_good_bets = np.median(np.array(results_True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_results_bad_bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00918cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta variable (gap between bad and good group) is the variable we aim to maximise with our features and models fine-tuning, the greater the better!\n",
    "delta = np.median(np.array(median_results_good_bets))-np.median(np.array(median_results_bad_bets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06315620",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_extremes = np.median(np.array(results_xgb_4))-np.median(np.array(results_xgb_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = np.median(np.array(results_xgb_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ca733",
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_results = np.median(np.array(results_xgb_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c92802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(delta, delta_extremes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgb_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of the XGB\n",
    "sns.histplot(results_True,kde=True,bins=20,color='green')\n",
    "sns.histplot(results_False,kde=True,bins=20,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of the XGB\n",
    "sns.histplot(results_xgb_true,kde=True,bins=20,color='green')\n",
    "sns.histplot(results_xgb_false,kde=True,bins=20,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01379d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of the XGB\n",
    "sns.histplot(results_xgb_1,kde=True,bins=20,color='red')\n",
    "sns.histplot(results_xgb_2,kde=True,bins=20,color='orange')\n",
    "sns.histplot(results_xgb_3,kde=True,bins=20,color='grey')\n",
    "sns.histplot(results_xgb_4,kde=True,bins=20,color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4279cf1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [SKIP] Comments regarding results of the tests we ran and Next Steps [SKIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd443a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Comments\n",
    "\n",
    "# baseline = 0.008297694850435258 (all features)\n",
    "\n",
    "# delta without Joao features = 0.014419296789770412 => F:-0.029970666611150436 and T:-0.015551369821380024\n",
    "# --> WE REMOVE JOAO FEATURE\n",
    "\n",
    "# delta when removing the odds buckets = -0.009317543146000251 => F:-0.022611272813043187 and T:-0.028804787343669917\n",
    "# --> WE KEEP THE ODDS BUCKETS\n",
    "\n",
    "# delta when removing the odds buckets = -0.009317543146000251 => F:-0.022611272813043187 and T:-0.028804787343669917\n",
    "# --> WE KEEP THE ODDS BUCKETS\n",
    "\n",
    "# delta when removing the countries and divisions = 0.004795286566703704\n",
    "# --> WE KEEP THE COUNTRIES AND DIVISIONS\n",
    "\n",
    "# delta when removing the divisions but keeping the countries = 0.008380480363148559 (worsening)\n",
    "# delta when removing the countries but keeping the divisions = 0.02259680830123266 (best score ever)\n",
    "# --> WE KEEP THE DIVSIONS AND REMOVE THE COUNTRIES\n",
    "\n",
    "# delta when removing the 'month_after_July': 0.013657154854343441 DECREASED A LOT! We keep month after July\n",
    "# year 2020_2021 = 0.012829398919170131 DECREASED A LOT! \n",
    "# TIME MATTERS!! We need months, years, time\n",
    "\n",
    "# New basline: 0.023917075840908224\n",
    "\n",
    "#'Pin pays better' does not improve the baseline and we remove it\n",
    "\n",
    "#'Market_consensus' MATTERS we keep it\n",
    "\n",
    "#VIG matters\n",
    "\n",
    "# We should remove the P<PC variable\n",
    "# BEST DELTA: 0.03151890594630853\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3ddff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Next steps:\n",
    "\n",
    "## Time: Further explore how to optimise the features of time (years, months, hours, etc.)\n",
    "## Odds: Further explore how to optimise the odds buckets (different bins, min-max scaling)\n",
    "## VIG + Mkt Consensus: Further explore how to optimise VIG + Market consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db8eb1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m.feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70fd7a8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# [SKIP] Other models + stats package we could use [SKIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c50b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1a60e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "my_model = xgb.XGBRegressor()\n",
    "my_model.fit(X_train_0,y_train_0) \n",
    "        \n",
    "        #Make and store the predictions XGBoost\n",
    "       # y_pred_xgb = m.predict(X_test)\n",
    "       # y_pred_xgb = pd.DataFrame(y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ac051",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#my_model = RandomForestRegressor(random_state=0).fit(X_step_joao, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92f724",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = my_model.predict(X_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1128b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4b403",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(X_test_0)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values, X_test_0)\n",
    "\n",
    "#- Vertical location shows what feature it is depicting\n",
    "#- Color shows whether that feature was high or low for that row of the dataset\n",
    "#- Horizontal location shows whether the effect of that value caused a higher or lower prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a37281",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test_0, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70af6d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
