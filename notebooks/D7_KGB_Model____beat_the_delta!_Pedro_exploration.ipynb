{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1816ff7",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cf7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e1f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b295e690",
   "metadata": {},
   "source": [
    "# Loading the Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4f75c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Merging the Seasons csv files (2019-2020 untill 2021-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b6d4af",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spain_2019_2020_1.csv', 'italy_2020_2021_2.csv', 'italy_2021_2022_1.csv', 'italy_2021_2022_2.csv', 'spain_2020_2021_1.csv', 'spain_2020_2021_2.csv', 'scotland_2020_2021_2.csv', 'Turkey_2021_2022_1.csv', 'scotland_2020_2021_3.csv', 'scotland_2019_2020_1.csv', 'france_2021_2022_1.csv', 'germany_2021_2022_1.csv', 'belgium_2020_2021_1.csv', 'scotland_2021_2022_4.csv', 'scotland_2020_2021_1.csv', 'france_2019_2020_1.csv', 'germany_2021_2022_2.csv', 'germany_2019_2020_1.csv', 'england_2020_2021_3.csv', 'scotland_2019_2020_2.csv', 'italy_2019_2020_1.csv', 'scotland_2020_2021_4.csv', 'portugal_2021_2022_1.csv', 'france_2020_2021_2.csv', 'scotland_2021_2022_3.csv', 'england_2021_2022_3.csv', 'portugal_2019_2020_1.csv', 'Greece_2021_2022_1.csv', 'england_2020_2021_4.csv', 'england_2021_2022_4.csv', 'france_2020_2021_1.csv', 'germany_2020_2021_1.csv', 'scotland_2019_2020_4.csv', 'spain_2019_20220_2.csv', 'germany_2019_2020_2.csv', 'england_2019_2020_4.csv', 'scotland_2021_2022_1.csv', 'england_2020_2021_2.csv', 'france_2019_2020_2.csv', 'Turkey_2019_2020_1.csv', 'england_2019_2020_2.csv', 'Eredivisie_2021_2022_1.csv', 'Turkey_2020_2021_1.csv', 'spain_2021_2022_1.csv', 'france_2021_2022_2.csv', 'belgium_2021_2022_1.csv', 'Greece_2019_2020_1.csv', 'belgium_2019_2020_1.csv', 'england_2019_2020_1.csv', 'Greece_2020_2021_1.csv', 'spain_2021_2022_2.csv', 'italy_2019_2020_2.csv', 'portugal_2020_2021_1.csv', 'Eredivisie_2020_2021_1.csv', 'italy_2020_2021_1.csv', 'Eredivisie_2019_2020_1.csv', 'england_2021_2022_2.csv', 'england_2020_2021_1.csv', 'england_2019_2020_3.csv', 'scotland_2021_2022_2.csv', 'scotland_2019_2020_3.csv', 'england_2021_2022_1.csv', 'germany_2020_2021_2.csv']\n"
     ]
    }
   ],
   "source": [
    "# Customise based on your path and folder organisation\n",
    "print(os.listdir('../OnThePitch/raw_data/All4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2208c966",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all the CSVs\n",
    "\n",
    "files = [file for file in os.listdir('../OnThePitch/raw_data/All4') if file.endswith('.csv')]\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv('../OnThePitch/raw_data/All4/' + file)\n",
    "    df['country']=str(file)[0:2]\n",
    "    df['country_division']=str(file)[-5:-4]\n",
    "    data = pd.concat([data, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8ad8d",
   "metadata": {},
   "source": [
    "# Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acad4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the function\n",
    "\n",
    "def feature_engineering(data, b=20, binned=False):\n",
    "    '''\n",
    "    This function creates all the columns that will be needed to create the analysis \n",
    "    and return the dataframe with all this changes\n",
    "    \n",
    "    b is the number of bins that we want to work with. Our start value for b will be 20.\n",
    "        '''\n",
    "    #------------------------Number of Goals, Over and Under -----------------------------------\n",
    "    \n",
    "    # total number of goals = goals from the home team + goals from visiting team\n",
    "    data['nb_goals']=data['FTHG']+data['FTAG']\n",
    "\n",
    "    # boolean: true or false regarding whether they were more than 2.5 goals\n",
    "    data['over_2.5_goals']=data['nb_goals']>2.5\n",
    "\n",
    "    # boolean: true or false regarding whether they were less than 2.5 goals\n",
    "    data['under_2.5_goals']=data['nb_goals']<2.5\n",
    "    \n",
    "    #-----------------------------Payout Opening ----------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average OPENING odds\n",
    "    data['payout_avg_under_2.5'] = data['under_2.5_goals']*data['Avg<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average OPENING odds\n",
    "    data['payout_avg_over_2.5'] = data['over_2.5_goals']*data['Avg>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE specifically\n",
    "    data['payout_under_2.5_pinacle'] = data['under_2.5_goals']*data['P<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE specifically\n",
    "    data['payout_over_2.5_pinacle'] = data['over_2.5_goals']*data['P>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 specifically\n",
    "    data['payout_under_2.5_365'] = data['under_2.5_goals']*data['B365<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 specifically\n",
    "    data['payout_over_2.5_365'] = data['over_2.5_goals']*data['B365>2.5']\n",
    "    \n",
    "    #------------------------------Payout Closing --------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_under_closing_2.5'] = data['under_2.5_goals']*data['AvgC<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_over_closing_2.5'] = data['over_2.5_goals']*data['AvgC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE closing ddds specifically\n",
    "    data['payout_under_2.5_pinacle_closing'] = data['under_2.5_goals']*data['PC<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE closing odds specifically\n",
    "    data['payout_over_2.5_pinacle_closing'] = data['over_2.5_goals']*data['PC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 closing odds specifically\n",
    "    data['payout_under_2.5_365_closing'] = data['under_2.5_goals']*data['B365C<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 closing odds specifically\n",
    "    data['payout_over_2.5_365_closing'] = data['over_2.5_goals']*data['B365C>2.5']\n",
    "    \n",
    "    #-------------------------- Implied Probability Opening ----------------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability <2.5 avg']=1/data['Avg<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability >2.5 avg']=1/data['Avg>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE\n",
    "    data['Implied Probability <2.5 pinacle']=1/data['P<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE\n",
    "    data['Implied Probability >2.5 pinacle']=1/data['P>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365\n",
    "    data['Implied Probability <2.5 365']=1/data['B365<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365\n",
    "    data['Implied Probability >2.5 365']=1/data['B365>2.5']*100\n",
    "    \n",
    "    #------------------------- Implied Probability Closing -----------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability <2.5 avg closing']=1/data['AvgC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability >2.5 avg closing']=1/data['AvgC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability <2.5 pinacle closing']=1/data['PC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability >2.5 pinacle closing']=1/data['PC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability <2.5 365 closing']=1/data['B365C<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability >2.5 365 closing']=1/data['B365C>2.5']*100\n",
    "    \n",
    "    #---------------------------- Binning IP Opening -------------------------------------\n",
    "\n",
    "    b=b\n",
    "    bins = np.arange(0, 101, int(100/b))\n",
    "    bins = bins.tolist()\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned <2.5 avg'] = pd.cut(data['Implied Probability <2.5 avg'], bins)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned >2.5 avg'] = pd.cut(data['Implied Probability >2.5 avg'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned <2.5 pinacle'] = pd.cut(data['Implied Probability <2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned >2.5 pinacle'] = pd.cut(data['Implied Probability >2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned <2.5 365'] = pd.cut(data['Implied Probability <2.5 365'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned >2.5 365'] = pd.cut(data['Implied Probability >2.5 365'], bins)\n",
    "    \n",
    "    #----------------------------- Binning IP Closing ------------------------------------------------\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market closing odds\n",
    "    data['binned <2.5 avg closing'] = pd.cut(data['Implied Probability <2.5 avg closing'], bins)\n",
    "\n",
    "    #Binning OVER 2.5 Average Market closing odds\n",
    "    data['binned >2.5 avg closing'] = pd.cut(data['Implied Probability >2.5 avg closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle closing odds\n",
    "    data['binned <2.5 pinacle closing'] = pd.cut(data['Implied Probability <2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle CLOSING odds\n",
    "    data['binned >2.5 pinacle closing'] = pd.cut(data['Implied Probability >2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 CLOSING odds\n",
    "    data['binned <2.5 365 closing'] = pd.cut(data['Implied Probability <2.5 365 closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 CLOSING odds\n",
    "    data['binned >2.5 365 closing'] = pd.cut(data['Implied Probability >2.5 365 closing'], bins)\n",
    "    \n",
    "    #---------------------------- Binning Odds Opening ----------------------------------------------------\n",
    "    \n",
    "    bins2 = [1, 1.5, 2, 3, 99999]\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg'] = pd.cut(data['Avg<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg'] = pd.cut(data['Avg>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle'] = pd.cut(data['P<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle'] = pd.cut(data['P>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365'] = pd.cut(data['B365<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365'] = pd.cut(data['B365>2.5'], bins2)\n",
    "    \n",
    "    #----------------------------- Binning Odds Closing ----------------------------------------------------------\n",
    "    \n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg closing'] = pd.cut(data['AvgC<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg closing'] = pd.cut(data['AvgC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle closing'] = pd.cut(data['PC<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle closing'] = pd.cut(data['PC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365 closing'] = pd.cut(data['B365C<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365 closing'] = pd.cut(data['B365C>2.5'], bins2)\n",
    "    \n",
    "    \n",
    "    #----------------------------- Other Features from D3 ------------------------------------------------------\n",
    "    \n",
    "    data['Pin_pays_better_under_boolean'] = data['PC<2.5'] > data['AvgC<2.5']\n",
    "    data['Pin_pays_better_under_difference'] = data['PC<2.5'] / data['AvgC<2.5']\n",
    "    data['%vig_p'] = (1 - (1 / (1/data['PC>2.5'] + 1/data['PC<2.5'])))*100\n",
    "    data['%vig_avg'] = (1 - (1 / (1/data['AvgC>2.5'] + 1/data['AvgC<2.5'])))*100\n",
    "    data['PC<2.5_P_boolean'] = data['PC<2.5'] < data['P<2.5']\n",
    "    data['PC<2.5_P_relative_diff'] = data['PC<2.5'] / data['P<2.5']\n",
    "    \n",
    "    #----------------------- Odds and probability of the home team scoring under 2.5 -------------------------------\n",
    "    \n",
    "#     lst1 = []\n",
    "#     lst2 = []\n",
    "#     for i, team in enumerate(data['HomeTeam']):\n",
    "#         date = data['Date'].iloc[i]\n",
    "#         total = len(data[(data['HomeTeam'] == team) & (data['Date'] < date)])\n",
    "#         n_under_home = data[(data['HomeTeam'] == team) & (data['Date'] < date)]['under_2.5_goals'].value_counts()\n",
    "#         try:\n",
    "#             lst1.append(1/(n_under_home[1]/total))\n",
    "#             lst2.append(n_under_home[1]/total)\n",
    "#         except:\n",
    "#             lst1.append(np.nan)\n",
    "#             lst2.append(np.nan)\n",
    "\n",
    "#     data['odds_home_under'] = lst1\n",
    "#     data['prob_home_under'] = lst2\n",
    "    \n",
    "#     #binning the probability of the home team to have a game of less than 2.5 score\n",
    "#     data['binned prob_home_under'] = pd.cut(data['prob_home_under']*100, bins)\n",
    "    \n",
    "    \n",
    "    #----------------------- Odds and probability of the away team scoring under 2.5 -------------------------------\n",
    "    \n",
    "#     lst3 = []\n",
    "#     lst4  = []\n",
    "#     for i, team in enumerate(data['AwayTeam']):\n",
    "#         date = data['Date'].iloc[i]\n",
    "#         total2 = len(data[(data['AwayTeam'] == team) & (data['Date'] < date)])\n",
    "#         n_under_away2 = data[(data['AwayTeam'] == team) & (data['Date'] < date)]['under_2.5_goals'].value_counts()\n",
    "#         try:\n",
    "#             lst3.append(1/(n_under_away2[1] / total2))\n",
    "#             lst4.append(n_under_away2[1] / total2)\n",
    "#         except:\n",
    "#             lst3.append(np.nan)\n",
    "#             lst4.append(np.nan)\n",
    "\n",
    "#     data['odds_away_under'] = lst3\n",
    "#     data['prob_away_under'] = lst4\n",
    "    \n",
    "#     #binning the probability of the away team to have a game of less than 2.5 score\n",
    "#     data['binned prob_away_under'] = pd.cut(data['prob_away_under']*100, bins)\n",
    "\n",
    "    #-------------------------- Creating the prob and odds of the game -----------------------------------------------\n",
    "#     '''the mean between the probability of the home team to have a score of under 2.5 and the probability \n",
    "#     of the away team to do the same'''\n",
    "    \n",
    "#     data['odds_game'] = (data['odds_away_under'] +  data['odds_home_under']) / 2\n",
    "#     data['prob_game'] = (data['prob_away_under'] + data['prob_home_under']) / 2\n",
    "    \n",
    "    #-------------------------- OneHotEncoding the binned probabilities columns ------------------------------------------\n",
    "    \n",
    "\n",
    "#     if b == 5:\n",
    "#         data = data[~data['binned prob_home_under'].isna()]\n",
    "#         ohe = OneHotEncoder(sparse=False)\n",
    "#         ohe.fit(data[['binned prob_home_under']])\n",
    "#         bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "#         data[\"0, 20\"], data[\"20, 40\"], data[\"40, 60\"], data[\"60, 80\"], data[\"80, 100\"] = bins_encoded.T\n",
    "        \n",
    "#     if b == 10:\n",
    "#         data = data[~data['binned prob_home_under'].isna()]\n",
    "#         ohe = OneHotEncoder(sparse=False)\n",
    "#         ohe.fit(data[['binned prob_home_under']])\n",
    "#         bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "#         data[\"0, 10\"], data[\"10, 20\"], data[\"20, 30\"], data[\"30, 40\"], data[\"40, 50\"], data[\"50, 60\"], \\\n",
    "#         data[\"60, 70\"], data[\"70, 80\"], data[\"80, 90\"], data[\"90, 100\"] = bins_encoded.T\n",
    "        \n",
    "#     if b == 20:\n",
    "#         data = data[~data['binned prob_home_under'].isna()]\n",
    "#         ohe = OneHotEncoder(sparse=False)\n",
    "#         ohe.fit(data[['binned prob_home_under']])\n",
    "#         bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "#         data[\"0, 5\"], data[\"5, 10\"], data[\"10, 15\"], data[\"15, 20\"], data[\"20, 25\"], data[\"25, 30\"], \\\n",
    "#         data[\"30, 35\"], data[\"35, 40\"], data[\"40, 45\"], data[\"45, 50\"], data[\"50, 55\"], data[\"55, 60\"], \\\n",
    "#         data[\"60, 65\"], data[\"65, 70\"], data[\"70, 75\"], data[\"75, 80\"], data[\"80, 85\"], data[\"85, 90\"], \\\n",
    "#         data[\"90, 95\"], data[\"95, 100\"]= bins_encoded.T\n",
    "    \n",
    "    #------------------------------------ Cleaning the data ---------------------------------------------------------\n",
    "    \n",
    "    #data = data.dropna(subset=['HomeTeam', 'AwayTeam'], how='any')\n",
    "    data = data[~data['HomeTeam'].isna()]\n",
    "    data = data[~data['AwayTeam'].isna()]\n",
    "    data = data[~data['PC>2.5'].isna()]\n",
    "    data.drop(columns=['Referee','Unnamed: 105'], inplace=True) #, 'Unnamed: 105' 'Referee', \n",
    "    #data.dropna()\n",
    "    \n",
    "     #-------------------------- OneHotEncoding the binned odds ------------------------------------------\n",
    "   \n",
    "    ohe = OneHotEncoder(sparse=False) \n",
    "    ohe.fit(data[['binned odds <2.5 pinacle closing']])\n",
    "    bins_encoded = ohe.transform(data[['binned odds <2.5 pinacle closing']])\n",
    "    data[\"1.0_to_1.5\"], data[\"1.5_to_2.0\"], data[\"2.0_to_3.0\"], data[\"3.0_to_99999.0\"] = bins_encoded.T\n",
    "    data.drop(columns='binned odds <2.5 pinacle closing', inplace=True)\n",
    "    \n",
    "    #-------------------------- OneHotEncoding the binned countries ------------------------------------------\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False) \n",
    "    ohe.fit(data[['country']])\n",
    "    bins_encoded = ohe.transform(data[['country']])\n",
    "    data[\"country_1\"], data[\"country_2\"], data[\"country_3\"], data[\"country_4\"], data[\"country_5\"],data[\"country_6\"], data[\"country_7\"], data[\"country_8\"], data[\"country_9\"], data[\"country_10\"], data[\"country_11\"] = bins_encoded.T\n",
    "    data.drop(columns='country', inplace=True)\n",
    "\n",
    "    #-------------------------- OneHotEncoding the binned country divisions ------------------------------------------\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse=False) \n",
    "    ohe.fit(data[['country_division']])\n",
    "    bins_encoded = ohe.transform(data[['country_division']])\n",
    "    data[\"country_div_1\"], data[\"country_div_2\"], data[\"country_div_3\"], data[\"country_div_4\"] = bins_encoded.T\n",
    "    data.drop(columns='country_division', inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c518df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the function and creating the dataset data\n",
    "\n",
    "data = feature_engineering(data, b=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d725b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WE WILL NEED TO ADD ALL THOSE IN THE feature_engineering FUNCTION\n",
    "\n",
    "## Adding the Year Feature \n",
    "data_date = data['Date']\n",
    "data_time = data['Time']\n",
    "data_date_2 = pd.to_datetime(data_date, dayfirst = True)\n",
    "data_time_2 = pd.to_datetime(data_time, dayfirst = True)\n",
    "data['month'] = pd.DatetimeIndex(data_date_2).month\n",
    "data['month_after_July'] = data['month']>=7\n",
    "data['year'] = pd.DatetimeIndex(data_date_2).year\n",
    "data['year_2021_2022'] = data['year']>=2021\n",
    "data['year_2022'] = data['year']>=2022\n",
    "data['year_2020'] = data['year']==2020\n",
    "data['season_21_22'] = data_date_2>='2021-09-01'\n",
    "data['hour'] = pd.DatetimeIndex(data_time_2).hour\n",
    "data['game_starts_after_4pm']=data['hour']>=16\n",
    "\n",
    "#Other features\n",
    "data['Pin_pays_better_under_boolean'] = data['PC<2.5'] > data['AvgC<2.5']\n",
    "data['Pin_pays_better_under_difference'] = data['PC<2.5'] / data['AvgC<2.5']\n",
    "data['%vig_p'] = (1 - (1 / (1/data['PC>2.5'] + 1/data['PC<2.5'])))*100\n",
    "data['%vig_p_bool'] = data['%vig_p']>3.3\n",
    "data['%vig_avg'] = (1 - (1 / (1/data['AvgC>2.5'] + 1/data['AvgC<2.5'])))*100\n",
    "data['PC<2.5_P_boolean'] = data['PC<2.5'] < data['P<2.5']\n",
    "data['PC<2.5_P_relative_diff'] = data['PC<2.5'] / data['P<2.5']\n",
    "data['MaxC>2.5_AvgC_relative_diff'] = data['MaxC>2.5']/data['AvgC>2.5']\n",
    "data['Market_consensus'] = data['MaxC>2.5_AvgC_relative_diff']<1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4275cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month_number']=data['month']/12\n",
    "data['month_number_ratio']=data['month']/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9d004d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['year_number'] = data['year'].map({2019:1, 2020:2, 2021:3, 2022:4})\n",
    "data['year_number_ratio'] = data['year'].map({2019:1, 2020:2, 2021:3, 2022:4})/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edf4c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>...</th>\n",
       "      <th>season_21_22</th>\n",
       "      <th>hour</th>\n",
       "      <th>game_starts_after_4pm</th>\n",
       "      <th>%vig_p_bool</th>\n",
       "      <th>MaxC&gt;2.5_AvgC_relative_diff</th>\n",
       "      <th>Market_consensus</th>\n",
       "      <th>month_number</th>\n",
       "      <th>month_number_ratio</th>\n",
       "      <th>year_number</th>\n",
       "      <th>year_number_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SP1</td>\n",
       "      <td>16/08/2019</td>\n",
       "      <td>20:00</td>\n",
       "      <td>Ath Bilbao</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.069892</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SP1</td>\n",
       "      <td>17/08/2019</td>\n",
       "      <td>16:00</td>\n",
       "      <td>Celta</td>\n",
       "      <td>Real Madrid</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.027211</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SP1</td>\n",
       "      <td>17/08/2019</td>\n",
       "      <td>18:00</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SP1</td>\n",
       "      <td>17/08/2019</td>\n",
       "      <td>19:00</td>\n",
       "      <td>Mallorca</td>\n",
       "      <td>Eibar</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SP1</td>\n",
       "      <td>17/08/2019</td>\n",
       "      <td>20:00</td>\n",
       "      <td>Leganes</td>\n",
       "      <td>Osasuna</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.059480</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>D2</td>\n",
       "      <td>23/05/2021</td>\n",
       "      <td>14:30</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>Nurnberg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.060150</td>\n",
       "      <td>False</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>D2</td>\n",
       "      <td>23/05/2021</td>\n",
       "      <td>14:30</td>\n",
       "      <td>Heidenheim</td>\n",
       "      <td>Karlsruhe</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.028369</td>\n",
       "      <td>True</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>D2</td>\n",
       "      <td>23/05/2021</td>\n",
       "      <td>14:30</td>\n",
       "      <td>Holstein Kiel</td>\n",
       "      <td>Darmstadt</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.045113</td>\n",
       "      <td>True</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>D2</td>\n",
       "      <td>23/05/2021</td>\n",
       "      <td>14:30</td>\n",
       "      <td>Regensburg</td>\n",
       "      <td>St Pauli</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.046667</td>\n",
       "      <td>True</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>D2</td>\n",
       "      <td>23/05/2021</td>\n",
       "      <td>14:30</td>\n",
       "      <td>Wurzburger Kickers</td>\n",
       "      <td>Paderborn</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.032258</td>\n",
       "      <td>True</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20905 rows × 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Div        Date   Time            HomeTeam     AwayTeam  FTHG  FTAG FTR  \\\n",
       "0    SP1  16/08/2019  20:00          Ath Bilbao    Barcelona     1     0   H   \n",
       "1    SP1  17/08/2019  16:00               Celta  Real Madrid     1     3   A   \n",
       "2    SP1  17/08/2019  18:00            Valencia     Sociedad     1     1   D   \n",
       "3    SP1  17/08/2019  19:00            Mallorca        Eibar     2     1   H   \n",
       "4    SP1  17/08/2019  20:00             Leganes      Osasuna     0     1   A   \n",
       "..   ...         ...    ...                 ...          ...   ...   ...  ..   \n",
       "301   D2  23/05/2021  14:30            Hannover     Nurnberg     1     2   A   \n",
       "302   D2  23/05/2021  14:30          Heidenheim    Karlsruhe     1     2   A   \n",
       "303   D2  23/05/2021  14:30       Holstein Kiel    Darmstadt     2     3   A   \n",
       "304   D2  23/05/2021  14:30          Regensburg     St Pauli     3     0   H   \n",
       "305   D2  23/05/2021  14:30  Wurzburger Kickers    Paderborn     1     1   D   \n",
       "\n",
       "     HTHG  HTAG  ... season_21_22  hour  game_starts_after_4pm  %vig_p_bool  \\\n",
       "0     0.0   0.0  ...        False    20                   True        False   \n",
       "1     0.0   1.0  ...        False    16                   True        False   \n",
       "2     0.0   0.0  ...        False    18                   True        False   \n",
       "3     1.0   0.0  ...        False    19                   True        False   \n",
       "4     0.0   0.0  ...        False    20                   True        False   \n",
       "..    ...   ...  ...          ...   ...                    ...          ...   \n",
       "301   1.0   1.0  ...        False    14                  False         True   \n",
       "302   0.0   1.0  ...        False    14                  False         True   \n",
       "303   1.0   0.0  ...        False    14                  False         True   \n",
       "304   2.0   0.0  ...        False    14                  False         True   \n",
       "305   0.0   1.0  ...        False    14                  False         True   \n",
       "\n",
       "     MaxC>2.5_AvgC_relative_diff  Market_consensus  month_number  \\\n",
       "0                       1.069892             False      0.666667   \n",
       "1                       1.027211              True      0.666667   \n",
       "2                       1.040000              True      0.666667   \n",
       "3                       1.062500             False      0.666667   \n",
       "4                       1.059480             False      0.666667   \n",
       "..                           ...               ...           ...   \n",
       "301                     1.060150             False      0.416667   \n",
       "302                     1.028369              True      0.416667   \n",
       "303                     1.045113              True      0.416667   \n",
       "304                     1.046667              True      0.416667   \n",
       "305                     1.032258              True      0.416667   \n",
       "\n",
       "     month_number_ratio  year_number  year_number_ratio  \n",
       "0              0.666667            1               0.25  \n",
       "1              0.666667            1               0.25  \n",
       "2              0.666667            1               0.25  \n",
       "3              0.666667            1               0.25  \n",
       "4              0.666667            1               0.25  \n",
       "..                  ...          ...                ...  \n",
       "301            0.416667            3               0.75  \n",
       "302            0.416667            3               0.75  \n",
       "303            0.416667            3               0.75  \n",
       "304            0.416667            3               0.75  \n",
       "305            0.416667            3               0.75  \n",
       "\n",
       "[20905 rows x 197 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3104f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hour_number'] = data['hour'].map({10:1, 11:2, 12:3, 13:4, 14:5, 15:6, 16:7, 17:8, 18:9, 19:10, 20:11, 21:12})/12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0df27",
   "metadata": {},
   "source": [
    "# Running the XGB model for under 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "899a0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model dataset with relevant variables\n",
    "data_linear_booleans_lean_P_under = data[['year_number','year_number_ratio','month_number','month_number_ratio','hour_number','country_div_1','country_div_2','country_div_3','country_div_4','month_after_July','season_21_22','year_2020','game_starts_after_4pm','Market_consensus','%vig_p_bool','1.0_to_1.5', '1.5_to_2.0', '2.0_to_3.0','3.0_to_99999.0','payout_under_2.5_pinacle_closing']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0bb8a",
   "metadata": {},
   "source": [
    "## Defining the function testing_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd3e732",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2783072612.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [15]\u001b[0;36m\u001b[0m\n\u001b[0;31m    y_train =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X_train = X1[X1['season_21_22']==False]\n",
    "X_test = X1[X1['season_21_22']==True]\n",
    "y_train =\n",
    "y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_lr_true = []\n",
    "# results_lr_false = []\n",
    "\n",
    "def testing_models(iterations):   \n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i < iterations:\n",
    "        # Split into Train/Test\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3) # Split into Train/Test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_test_0, y_test_0, test_size=test_size_0) # Split into Train/Test\n",
    "\n",
    "            #------------------------Run the Models -----------------------------------\n",
    "\n",
    "        #Run Linear Regression\n",
    "#         results_linear_regression = sm.OLS(y_train,X_train.astype(float)).fit()\n",
    "#         y_pred_lr = results_linear_regression.predict(X_test.astype(float))\n",
    "#         #results.summary()\n",
    "\n",
    "        #Initiate XGBoost\n",
    "        m = 0\n",
    "        m = xgb.XGBRegressor()\n",
    "        \n",
    "        #Rename columns for XGBoost to run\n",
    "        ##X_test.rename(columns = {'PC<2.5_P_boolean':'PC_under_2.5_P_boolean'}, inplace = True)\n",
    "        ##X_train.rename(columns = {'PC<2.5_P_boolean':'PC_under_2.5_P_boolean'}, inplace = True)\n",
    "        \n",
    "        #Fit XGBoost\n",
    "        m.fit(X_train_0,y_train_0) \n",
    "        \n",
    "        #Make and store the predictions XGBoost\n",
    "        y_pred_xgb = m.predict(X_test)\n",
    "        y_pred_xgb = pd.DataFrame(y_pred_xgb)\n",
    "\n",
    "            #------------------------Creating the bins for the predictions for both models -----------------------------------\n",
    "        #Function to replace bin with default value when it is null\n",
    "        def ifnull(var, val):\n",
    "          if var > 0:\n",
    "            return var\n",
    "          return val\n",
    "\n",
    "        #Linear Regression\n",
    "#         y_pred_lr_under_0_median = y_pred_lr[y_pred_lr<0].median()\n",
    "#         y_pred_lr_under_0_min = y_pred_lr[y_pred_lr<0].min()\n",
    "#         y_pred_lr_over_0_median = ifnull(y_pred_lr[y_pred_lr>0].median(),0.05)\n",
    "\n",
    "        #XGB bins\n",
    "        y_pred_xgb_under_0_median = y_pred_xgb[y_pred_xgb<0].median()\n",
    "        y_pred_xgb_under_0_min = y_pred_xgb[y_pred_xgb<0].min()\n",
    "        y_pred_xgb_over_0_median = ifnull(y_pred_xgb[0][y_pred_xgb[0]>0].median(),0.05)\n",
    "\n",
    "            #------------------------Betting decisions for both models -----------------------------------\n",
    "#         #Linear Regression\n",
    "#         bins3_lr = [y_pred_lr_under_0_min-0.0000002, y_pred_lr_under_0_median-0.0000001, 0, y_pred_lr_over_0_median+0.0000001, 1]\n",
    "#         y_lr_df = pd.DataFrame(y_test)\n",
    "#         y_pred_lr_df = pd.DataFrame(y_pred_lr)\n",
    "#         y_pred_lr_df[\"binned_pred\"] = pd.cut(y_pred_lr_df[0], bins3_lr)\n",
    "#         ind = np.arange(0, len(y_pred_lr_df))\n",
    "#         ind = ind.tolist()\n",
    "#         y_lr_df['ind'] = ind\n",
    "#         y_pred_lr_df['ind'] = ind\n",
    "#         y_final_lr = y_lr_df.merge(y_pred_lr_df, on=\"ind\")#, on = \"axis\")#, how = \"inner\")\n",
    "#         y_final_lr['bet_opp']=y_final_lr[0]>0\n",
    "\n",
    "        #XGB\n",
    "        bins3_xgb = [y_pred_xgb_under_0_min[0]-0.0000002, y_pred_xgb_under_0_median[0]-0.0000001, 0, y_pred_xgb_over_0_median+0.0000001, 1] #int(y_pred_xgb_over_0_median[0])\n",
    "        y_xgb_df = pd.DataFrame(y_test)\n",
    "        y_pred_xgb_df = pd.DataFrame(y_pred_xgb)\n",
    "        y_pred_xgb_df[\"binned_pred\"] = pd.cut(y_pred_xgb_df[0], bins3_xgb)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_1\"] = y_pred_xgb_df[0]<y_pred_xgb_under_0_median[0]-0.0000001\n",
    "        y_pred_xgb_df[\"binned_pred_bin_2\"] = (y_pred_xgb_df[0]>y_pred_xgb_under_0_median[0]-0.0000001) & (y_pred_xgb_df[0]<0)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_3\"] = (y_pred_xgb_df[0]>0) & (y_pred_xgb_df[0]<y_pred_xgb_over_0_median+0.0000001)\n",
    "        y_pred_xgb_df[\"binned_pred_bin_4\"] = (y_pred_xgb_df[0]>y_pred_xgb_over_0_median+0.0000001)\n",
    "        y_pred_xgb_df[\"bin_number\"] = y_pred_xgb_df[\"binned_pred_bin_1\"]*1 + y_pred_xgb_df[\"binned_pred_bin_2\"]*2 + y_pred_xgb_df[\"binned_pred_bin_3\"]*3 + y_pred_xgb_df[\"binned_pred_bin_4\"]*4     \n",
    "        ind = np.arange(0, len(y_pred_xgb_df))\n",
    "        ind = ind.tolist()\n",
    "        y_xgb_df['ind'] = ind\n",
    "        y_pred_xgb_df['ind'] = ind\n",
    "        y_final_xgb = y_xgb_df.merge(y_pred_xgb_df, on=\"ind\")#, on = \"axis\")#, how = \"inner\")\n",
    "        \n",
    "        #Defining the variable bet_opp based on the y_pred of the XGB\n",
    "        y_final_xgb['bet_opp']=y_final_xgb[0]>0\n",
    "\n",
    "            #------------------------Getting the results based on predictions -----------------------------------\n",
    "        #Linear Regression\n",
    "#         y_final_lr.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].count()\n",
    "#         y_final_lr.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].agg([\"mean\", \"count\"])\n",
    "\n",
    "#         y_final_lr.groupby('binned_pred')['payout_under_2.5_pinacle_closing'].count()\n",
    "#         y_final_lr.groupby('binned_pred')['payout_under_2.5_pinacle_closing'].agg([\"mean\", \"count\"])\n",
    "#         lr_results_False = y_final_lr[y_final_lr['bet_opp']==False]['payout_under_2.5_pinacle_closing'].mean()\n",
    "#         lr_results_True = y_final_lr[y_final_lr['bet_opp']==True]['payout_under_2.5_pinacle_closing'].mean()        \n",
    "        \n",
    "\n",
    "        #XGB\n",
    "        #xgb_count = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].count()\n",
    "        #xgb_mean = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].mean()\n",
    "        #xgb_results = y_final_xgb.groupby('bet_opp')['payout_under_2.5_pinacle_closing'].agg([\"mean\", \"count\"])\n",
    "        xgb_results_False = y_final_xgb[y_final_xgb['bet_opp']==False]['payout_under_2.5_pinacle_closing'].mean()\n",
    "        xgb_results_True = y_final_xgb[y_final_xgb['bet_opp']==True]['payout_under_2.5_pinacle_closing'].mean()\n",
    "        xgb_results_bin_1 = y_final_xgb[y_final_xgb['bin_number']==1]['payout_under_2.5_pinacle_closing'].mean()\n",
    "        xgb_results_bin_2 = y_final_xgb[y_final_xgb['bin_number']==2]['payout_under_2.5_pinacle_closing'].mean()\n",
    "        xgb_results_bin_3 = y_final_xgb[y_final_xgb['bin_number']==3]['payout_under_2.5_pinacle_closing'].mean()\n",
    "        xgb_results_bin_4 = y_final_xgb[y_final_xgb['bin_number']==4]['payout_under_2.5_pinacle_closing'].mean()\n",
    "        \n",
    "        results_xgb_false.append(xgb_results_False)\n",
    "        results_xgb_true.append(xgb_results_True)\n",
    "        results_xgb_1.append(xgb_results_bin_1)\n",
    "        results_xgb_2.append(xgb_results_bin_2)\n",
    "        results_xgb_3.append(xgb_results_bin_3)\n",
    "        results_xgb_4.append(xgb_results_bin_4)\n",
    "        \n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "iterables = [[feature_1], [feature_2], [feature_3]]\n",
    "\n",
    "for t in itertools.product(*iterables):\n",
    "    print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = ['1.0_to_1.5', '1.5_to_2.0', '2.0_to_3.0','3.0_to_99999.0']\n",
    "feature_2 = ['country_div_1','country_div_2','country_div_3','country_div_4']\n",
    "feature_3 = ['month_after_July']\n",
    "feature_4 = ['year_2021_2022']\n",
    "feature_5 = ['game_starts_after_4pm']\n",
    "feature_6 = ['Market_consensus']\n",
    "feature_7 = ['%vig_p_bool']\n",
    "feature_8 = ['payout_under_2.5_pinacle_closing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae79b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = [feature_1,feature_2,feature_3,feature_4,feature_5,feature_6,feature_7,feature_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "sample_list = list_features\n",
    "list_combinations = list()\n",
    "for n in range(len(sample_list) + 1):\n",
    "    list_combinations += list(combinations(sample_list, n))\n",
    "print(list_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98efc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ccea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(list_combinations)\n",
    "flat_ls = [item for sublist in ls for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ee716",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(list_combinations[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = list(list_combinations[3])\n",
    "[item[0] for item in mylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features_array = np.array(list_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5327fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(x[0]*list_features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "n = 2\n",
    "\n",
    "x = product([1, 0], repeat=n)\n",
    "x = np.reshape(list(x), (-1, n))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "#while i < 3:\n",
    "feature_selection_1 = x[0]*list_features_array[i]\n",
    "i=i=1\n",
    "final_feature_selection = feature_selection_1+feature_selection_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c473cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_feature_selection = feature_1*0+feature_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d491cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dac4f9",
   "metadata": {},
   "source": [
    "## Running the function testing_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select here the variables you want to include in the testing\n",
    "X_train_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==False][['country_div_1','country_div_2','country_div_3','country_div_4','month_after_July','game_starts_after_4pm','Market_consensus','%vig_p_bool','1.0_to_1.5', '1.5_to_2.0', '2.0_to_3.0','3.0_to_99999.0','year_number_ratio','month_number_ratio']]\n",
    "X_test_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==True][['country_div_1','country_div_2','country_div_3','country_div_4','month_after_July','game_starts_after_4pm','Market_consensus','%vig_p_bool','1.0_to_1.5', '1.5_to_2.0', '2.0_to_3.0','3.0_to_99999.0','year_number_ratio','month_number_ratio']]\n",
    "y_train_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==False]['payout_under_2.5_pinacle_closing']-1\n",
    "y_test_0 = data_linear_booleans_lean_P_under[data_linear_booleans_lean_P_under['season_21_22']==True]['payout_under_2.5_pinacle_closing']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function testing_models\n",
    "results_xgb_true = [] \n",
    "results_xgb_false = []\n",
    "results_xgb_1=[]\n",
    "results_xgb_2=[]\n",
    "results_xgb_3=[]\n",
    "results_xgb_4=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size_0=0.90\n",
    "testing_models(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of the X tests with XGB (that you ran with the function testing_models)\n",
    "median_results_bad_bets = np.median(np.array(results_xgb_false))\n",
    "median_results_good_bets = np.median(np.array(results_xgb_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83750f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_results_bad_bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00918cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta variable (gap between bad and good group) is the variable we aim to maximise with our features and models fine-tuning, the greater the better!\n",
    "delta = np.median(np.array(median_results_good_bets))-np.median(np.array(median_results_bad_bets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06315620",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_extremes = np.median(np.array(results_xgb_4))-np.median(np.array(results_xgb_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = np.median(np.array(results_xgb_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ca733",
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_results = np.median(np.array(results_xgb_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c92802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(delta, delta_extremes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f133f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_xgb_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of the XGB\n",
    "sns.histplot(results_xgb_true,kde=True,bins=20,color='green')\n",
    "sns.histplot(results_xgb_false,kde=True,bins=20,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01379d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of the XGB\n",
    "sns.histplot(results_xgb_1,kde=True,bins=20,color='red')\n",
    "sns.histplot(results_xgb_2,kde=True,bins=20,color='orange')\n",
    "sns.histplot(results_xgb_3,kde=True,bins=20,color='grey')\n",
    "sns.histplot(results_xgb_4,kde=True,bins=20,color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4279cf1",
   "metadata": {},
   "source": [
    "# Comments regarding results of the tests we ran and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd443a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comments\n",
    "\n",
    "# baseline = 0.008297694850435258 (all features)\n",
    "\n",
    "# delta without Joao features = 0.014419296789770412 => F:-0.029970666611150436 and T:-0.015551369821380024\n",
    "# --> WE REMOVE JOAO FEATURE\n",
    "\n",
    "# delta when removing the odds buckets = -0.009317543146000251 => F:-0.022611272813043187 and T:-0.028804787343669917\n",
    "# --> WE KEEP THE ODDS BUCKETS\n",
    "\n",
    "# delta when removing the odds buckets = -0.009317543146000251 => F:-0.022611272813043187 and T:-0.028804787343669917\n",
    "# --> WE KEEP THE ODDS BUCKETS\n",
    "\n",
    "# delta when removing the countries and divisions = 0.004795286566703704\n",
    "# --> WE KEEP THE COUNTRIES AND DIVISIONS\n",
    "\n",
    "# delta when removing the divisions but keeping the countries = 0.008380480363148559 (worsening)\n",
    "# delta when removing the countries but keeping the divisions = 0.02259680830123266 (best score ever)\n",
    "# --> WE KEEP THE DIVSIONS AND REMOVE THE COUNTRIES\n",
    "\n",
    "# delta when removing the 'month_after_July': 0.013657154854343441 DECREASED A LOT! We keep month after July\n",
    "# year 2020_2021 = 0.012829398919170131 DECREASED A LOT! \n",
    "# TIME MATTERS!! We need months, years, time\n",
    "\n",
    "# New basline: 0.023917075840908224\n",
    "\n",
    "#'Pin pays better' does not improve the baseline and we remove it\n",
    "\n",
    "#'Market_consensus' MATTERS we keep it\n",
    "\n",
    "#VIG matters\n",
    "\n",
    "# We should remove the P<PC variable\n",
    "# BEST DELTA: 0.03151890594630853\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps:\n",
    "\n",
    "## Time: Further explore how to optimise the features of time (years, months, hours, etc.)\n",
    "## Odds: Further explore how to optimise the odds buckets (different bins, min-max scaling)\n",
    "## VIG + Mkt Consensus: Further explore how to optimise VIG + Market consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70fd7a8",
   "metadata": {},
   "source": [
    "# [SKIP] Other models + stats package we could use [SKIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ac051",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = RandomForestRegressor(random_state=0).fit(X_step_joao, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model.predict(X_step_joao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(X_step_joao)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values, X_step_joao)\n",
    "\n",
    "#- Vertical location shows what feature it is depicting\n",
    "#- Color shows whether that feature was high or low for that row of the dataset\n",
    "#- Horizontal location shows whether the effect of that value caused a higher or lower prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a37281",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_step_joao, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
