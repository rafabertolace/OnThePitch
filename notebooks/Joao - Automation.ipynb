{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4431028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d6d0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3f968",
   "metadata": {},
   "source": [
    "## Creating the functiong that will get the data for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af396165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(league1, wall=False):\n",
    "    \n",
    "    if wall:\n",
    "        data = pd.DataFrame()\n",
    "        leagues = listdir(f'./../raw_data/')\n",
    "        data = pd.DataFrame()\n",
    "        for league in leagues:\n",
    "            files = listdir(f'./../raw_data/{league}')\n",
    "            for file in files:\n",
    "                df = pd.read_csv((f'./../raw_data/{league}/'+file))\n",
    "                data = pd.concat([data, df])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        files = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        for file in files:\n",
    "            df = pd.read_csv(f'./../raw_data/{league1}/'+file)\n",
    "            data = pd.concat([data, df])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3685cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_simple():\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    leagues = [file for file in listdir('./../raw_data')]\n",
    "    for league in leagues:\n",
    "        files = [file for file in listdir(f'./../raw_data/{league}')]\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv('./../raw_data/Turkey/'+file)\n",
    "        data = pd.concat([data, df])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698e8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_5(league1, league2=None, league3=None, league4=None, League5=None):\n",
    "    \n",
    "    files = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    #For the case we have 2 leagues to concatenate:\n",
    "    if league2:\n",
    "        files1 = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "        files2 = [file for file in listdir(f'./../raw_data/{league2}')]\n",
    "        for file1 in files1:\n",
    "            df = pd.read_csv(f'./../raw_data/{league1}/'+file1)\n",
    "            data = pd.concat([data, df])\n",
    "        for file2 in files2:\n",
    "            df = pd.read_csv(f'./../raw_data/{league2}/'+file2)\n",
    "            data = pd.concat([data, df])\n",
    "        return data\n",
    "    \n",
    "    #For the case we have 3 leagues to concatenate:\n",
    "    if league2 and league3:\n",
    "        files1 = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "        files2 = [file for file in listdir(f'./../raw_data/{league2}')]\n",
    "        files3 = [file for file in listdir(f'./../raw_data/{league3}')]\n",
    "        \n",
    "        for file1 in files1:\n",
    "            df = pd.read_csv(f'./../raw_data/{league1}/'+file1)\n",
    "            data = pd.concat([data, df])\n",
    "        for file2 in files2:\n",
    "            df = pd.read_csv(f'./../raw_data/{league2}/'+file2)\n",
    "            data = pd.concat([data, df])\n",
    "        for file3 in files3:\n",
    "            df = pd.read_csv(f'./../raw_data/{league3}/'+file3)\n",
    "            data = pd.concat([data, df])\n",
    "        return data\n",
    "    \n",
    "    #For the case we have 4 leagues to concatenate:\n",
    "    if league2 and league3 and league4:\n",
    "        files1 = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "        files2 = [file for file in listdir(f'./../raw_data/{league2}')]\n",
    "        files3 = [file for file in listdir(f'./../raw_data/{league3}')]\n",
    "        files4 = [file for file in listdir(f'./../raw_data/{league4}')]\n",
    "        \n",
    "        for file1 in files1:\n",
    "            df = pd.read_csv(f'./../raw_data/{league1}/'+file1)\n",
    "            data = pd.concat([data, df])\n",
    "        for file2 in files2:\n",
    "            df = pd.read_csv(f'./../raw_data/{league2}/'+file2)\n",
    "            data = pd.concat([data, df])\n",
    "        for file3 in files3:\n",
    "            df = pd.read_csv(f'./../raw_data/{league3}/'+file3)\n",
    "            data = pd.concat([data, df])\n",
    "        for file4 in files4:\n",
    "            df = pd.read_csv(f'./../raw_data/{league4}/'+file4)\n",
    "            data = pd.concat([data, df])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    #For the case we have 5 leagues to concatenate:\n",
    "    if league2 and league3 and league4 and league5:\n",
    "        files1 = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "        files2 = [file for file in listdir(f'./../raw_data/{league2}')]\n",
    "        files3 = [file for file in listdir(f'./../raw_data/{league3}')]\n",
    "        files4 = [file for file in listdir(f'./../raw_data/{league4}')]\n",
    "        files5 = [file for file in listdir(f'./../raw_data/{league5}')]\n",
    "        \n",
    "        for file1 in files1:\n",
    "            df = pd.read_csv(f'./../raw_data/{league1}/'+file1)\n",
    "            data = pd.concat([data, df])\n",
    "        for file2 in files2:\n",
    "            df = pd.read_csv(f'./../raw_data/{league2}/'+file2)\n",
    "            data = pd.concat([data, df])\n",
    "        for file3 in files3:\n",
    "            df = pd.read_csv(f'./../raw_data/{league3}/'+file3)\n",
    "            data = pd.concat([data, df])\n",
    "        for file4 in files4:\n",
    "            df = pd.read_csv(f'./../raw_data/{league4}/'+file4)\n",
    "            data = pd.concat([data, df])\n",
    "        for file5 in files5:\n",
    "            df = pd.read_csv(f'./../raw_data/{league5}/'+file5)\n",
    "            data = pd.concat([data, df])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bec63e",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72acb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data, b=20, binned=False):\n",
    "    '''\n",
    "    This function creates all the columns that will be needed to create the analysis \n",
    "    and return the dataframe with all this changes\n",
    "    \n",
    "    b is the number of bins that we want to work with. Our start value for b will be 20.\n",
    "        '''\n",
    "    #------------------------Number of Goals, Over and Under -----------------------------------\n",
    "    \n",
    "    # total number of goals = goals from the home team + goals from visiting team\n",
    "    data['nb_goals']=data['FTHG']+data['FTAG']\n",
    "\n",
    "    # boolean: true or false regarding whether they were more than 2.5 goals\n",
    "    data['over_2.5_goals']=data['nb_goals']>2.5\n",
    "\n",
    "    # boolean: true or false regarding whether they were less than 2.5 goals\n",
    "    data['under_2.5_goals']=data['nb_goals']<2.5\n",
    "    \n",
    "    #-----------------------------Payout Opening ----------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average OPENING odds\n",
    "    data['payout_avg_under_2.5'] = data['under_2.5_goals']*data['Avg<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average OPENING odds\n",
    "    data['payout_avg_over_2.5'] = data['over_2.5_goals']*data['Avg>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE specifically\n",
    "    data['payout_under_2.5_pinacle'] = data['under_2.5_goals']*data['P<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE specifically\n",
    "    data['payout_over_2.5_pinacle'] = data['over_2.5_goals']*data['P>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 specifically\n",
    "    data['payout_under_2.5_365'] = data['under_2.5_goals']*data['B365<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 specifically\n",
    "    data['payout_over_2.5_365'] = data['over_2.5_goals']*data['B365>2.5']\n",
    "    \n",
    "    #------------------------------Payout Closing --------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_under_closing_2.5'] = data['under_2.5_goals']*data['AvgC<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_over_closing_2.5'] = data['over_2.5_goals']*data['AvgC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE closing ddds specifically\n",
    "    data['payout_under_2.5_pinacle_closing'] = data['under_2.5_goals']*data['PC<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE closing odds specifically\n",
    "    data['payout_over_2.5_pinacle_closing'] = data['over_2.5_goals']*data['PC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 closing odds specifically\n",
    "    data['payout_under_2.5_365_closing'] = data['under_2.5_goals']*data['B365C<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 closing odds specifically\n",
    "    data['payout_over_2.5_365_closing'] = data['over_2.5_goals']*data['B365C>2.5']\n",
    "    \n",
    "    #-------------------------- Implied Probability Opening ----------------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability <2.5 avg']=1/data['Avg<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability >2.5 avg']=1/data['Avg>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE\n",
    "    data['Implied Probability <2.5 pinacle']=1/data['P<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE\n",
    "    data['Implied Probability >2.5 pinacle']=1/data['P>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365\n",
    "    data['Implied Probability <2.5 365']=1/data['B365<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365\n",
    "    data['Implied Probability >2.5 365']=1/data['B365>2.5']*100\n",
    "    \n",
    "    #------------------------- Implied Probability Closing -----------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability <2.5 avg closing']=1/data['AvgC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability >2.5 avg closing']=1/data['AvgC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability <2.5 pinacle closing']=1/data['PC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability >2.5 pinacle closing']=1/data['PC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability <2.5 365 closing']=1/data['B365C<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability >2.5 365 closing']=1/data['B365C>2.5']*100\n",
    "    \n",
    "    #---------------------------- Binning IP Opening -------------------------------------\n",
    "\n",
    "    b=b\n",
    "    bins = np.arange(0, 101, int(100/b))\n",
    "    bins = bins.tolist()\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned <2.5 avg'] = pd.cut(data['Implied Probability <2.5 avg'], bins)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned >2.5 avg'] = pd.cut(data['Implied Probability >2.5 avg'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned <2.5 pinacle'] = pd.cut(data['Implied Probability <2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned >2.5 pinacle'] = pd.cut(data['Implied Probability >2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned <2.5 365'] = pd.cut(data['Implied Probability <2.5 365'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned >2.5 365'] = pd.cut(data['Implied Probability >2.5 365'], bins)\n",
    "    \n",
    "    #----------------------------- Binning IP Closing ------------------------------------------------\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market closing odds\n",
    "    data['binned <2.5 avg closing'] = pd.cut(data['Implied Probability <2.5 avg closing'], bins)\n",
    "\n",
    "    #Binning OVER 2.5 Average Market closing odds\n",
    "    data['binned >2.5 avg closing'] = pd.cut(data['Implied Probability >2.5 avg closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle closing odds\n",
    "    data['binned <2.5 pinacle closing'] = pd.cut(data['Implied Probability <2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle CLOSING odds\n",
    "    data['binned >2.5 pinacle closing'] = pd.cut(data['Implied Probability >2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 CLOSING odds\n",
    "    data['binned <2.5 365 closing'] = pd.cut(data['Implied Probability <2.5 365 closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 CLOSING odds\n",
    "    data['binned >2.5 365 closing'] = pd.cut(data['Implied Probability >2.5 365 closing'], bins)\n",
    "    \n",
    "    #---------------------------- Binning Odds Opening ----------------------------------------------------\n",
    "    \n",
    "    bins2 = [1, 1.5, 2, 3, 99999]\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg'] = pd.cut(data['Avg<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg'] = pd.cut(data['Avg>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle'] = pd.cut(data['P<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle'] = pd.cut(data['P>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365'] = pd.cut(data['B365<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365'] = pd.cut(data['B365>2.5'], bins2)\n",
    "    \n",
    "    #----------------------------- Binning Odds Closing ----------------------------------------------------------\n",
    "    \n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg closing'] = pd.cut(data['AvgC<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg closing'] = pd.cut(data['AvgC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle closing'] = pd.cut(data['PC<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle closing'] = pd.cut(data['PC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365 closing'] = pd.cut(data['B365C<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365 closing'] = pd.cut(data['B365C>2.5'], bins2)\n",
    "    \n",
    "    \n",
    "    #----------------------------- Other Features from D3 ------------------------------------------------------\n",
    "    \n",
    "    data['Pin_pays_better_under_boolean'] = data['PC<2.5'] > data['AvgC<2.5']\n",
    "    data['Pin_pays_better_under_difference'] = data['PC<2.5'] / data['AvgC<2.5']\n",
    "    data['%vig_p'] = (1 - (1 / (1/data['PC>2.5'] + 1/data['PC<2.5'])))*100\n",
    "    data['%vig_avg'] = (1 - (1 / (1/data['AvgC>2.5'] + 1/data['AvgC<2.5'])))*100\n",
    "    data['PC<2.5_P_boolean'] = data['PC<2.5'] < data['P<2.5']\n",
    "    data['PC<2.5_P_relative_diff'] = data['PC<2.5'] / data['P<2.5']\n",
    "    \n",
    "    #----------------------- Odds and probability of the home team scoring under 2.5 -------------------------------\n",
    "    \n",
    "    lst1 = []\n",
    "    lst2 = []\n",
    "    for i, team in enumerate(data['HomeTeam']):\n",
    "        date = data['Date'].iloc[i]\n",
    "        total = len(data[(data['HomeTeam'] == team) & (data['Date'] < date)])\n",
    "        n_under_home = data[(data['HomeTeam'] == team) & (data['Date'] < date)]['under_2.5_goals'].value_counts()\n",
    "        try:\n",
    "            lst1.append(1/(n_under_home[1]/total))\n",
    "            lst2.append(n_under_home[1]/total)\n",
    "        except:\n",
    "            lst1.append(np.nan)\n",
    "            lst2.append(np.nan)\n",
    "\n",
    "    data['odds_home_under'] = lst1\n",
    "    data['prob_home_under'] = lst2\n",
    "    \n",
    "    #binning the probability of the home team to have a game of less than 2.5 score\n",
    "    data['binned prob_home_under'] = pd.cut(data['prob_home_under']*100, bins)\n",
    "    \n",
    "    \n",
    "    #----------------------- Odds and probability of the away team scoring under 2.5 -------------------------------\n",
    "    \n",
    "    lst3 = []\n",
    "    lst4  = []\n",
    "    for i, team in enumerate(data['AwayTeam']):\n",
    "        date = data['Date'].iloc[i]\n",
    "        total2 = len(data[(data['AwayTeam'] == team) & (data['Date'] < date)])\n",
    "        n_under_away2 = data[(data['AwayTeam'] == team) & (data['Date'] < date)]['under_2.5_goals'].value_counts()\n",
    "        try:\n",
    "            lst3.append(1/(n_under_away2[1] / total2))\n",
    "            lst4.append(n_under_away2[1] / total2)\n",
    "        except:\n",
    "            lst3.append(np.nan)\n",
    "            lst4.append(np.nan)\n",
    "\n",
    "    data['odds_away_under'] = lst3\n",
    "    data['prob_away_under'] = lst4\n",
    "    \n",
    "    #binning the probability of the away team to have a game of less than 2.5 score\n",
    "    data['binned prob_away_under'] = pd.cut(data['prob_away_under']*100, bins)\n",
    "\n",
    "    #-------------------------- Creating the prob and odds of the game -----------------------------------------------\n",
    "    '''the mean between the probability of the home team to have a score of under 2.5 and the probability \n",
    "    of the away team to do the same'''\n",
    "    \n",
    "    data['odds_game'] = (data['odds_away_under'] +  data['odds_home_under']) / 2\n",
    "    data['prob_game'] = (data['prob_away_under'] + data['prob_home_under']) / 2\n",
    "    \n",
    "    #-------------------------- OneHotEncoding the binned probabilities columns ------------------------------------------\n",
    "    \n",
    "\n",
    "    if b == 5:\n",
    "        data = data[~data['binned prob_home_under'].isna()]\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe.fit(data[['binned prob_home_under']])\n",
    "        bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "        data[\"0, 20\"], data[\"20, 40\"], data[\"40, 60\"], data[\"60, 80\"], data[\"80, 100\"] = bins_encoded.T\n",
    "        \n",
    "    if b == 10:\n",
    "        data = data[~data['binned prob_home_under'].isna()]\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe.fit(data[['binned prob_home_under']])\n",
    "        bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "        data[\"0, 10\"], data[\"10, 20\"], data[\"20, 30\"], data[\"30, 40\"], data[\"40, 50\"], data[\"50, 60\"], \\\n",
    "        data[\"60, 70\"], data[\"70, 80\"], data[\"80, 90\"], data[\"90, 100\"] = bins_encoded.T\n",
    "        \n",
    "    if b == 20:\n",
    "        data = data[~data['binned prob_home_under'].isna()]\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe.fit(data[['binned prob_home_under']])\n",
    "        bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "        data[\"0, 5\"], data[\"5, 10\"], data[\"10, 15\"], data[\"15, 20\"], data[\"20, 25\"], data[\"25, 30\"], \\\n",
    "        data[\"30, 35\"], data[\"35, 40\"], data[\"40, 45\"], data[\"45, 50\"], data[\"50, 55\"], data[\"55, 60\"], \\\n",
    "        data[\"60, 65\"], data[\"65, 70\"], data[\"70, 75\"], data[\"75, 80\"], data[\"80, 85\"], data[\"85, 90\"], \\\n",
    "        data[\"90, 95\"], data[\"95, 100\"]= bins_encoded.T\n",
    "    \n",
    "    \n",
    "    #------------------------------------ Cleaning the data ---------------------------------------------------------\n",
    "    \n",
    "    #data = data.dropna(subset=['HomeTeam', 'AwayTeam'], how='any')\n",
    "    data = data[~data['HomeTeam'].isna()]\n",
    "    data = data[~data['AwayTeam'].isna()]\n",
    "    data = data[~data['PC>2.5'].isna()]\n",
    "    data.drop(columns=['Referee', 'Unnamed: 105'], inplace=True)\n",
    "    #data.dropna()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab978404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dfs(data, pinnacle=True, bet365=True):\n",
    "    \n",
    "    if pinnacle == True and bet365 == True:\n",
    "        fdf = data[['Implied Probability','over_2.5_goals','binned','payout']]\n",
    "        fdf_under = data[['Implied Probability <2.5','under_2.5_goals','binned <2.5','payout_under_2.5']]\n",
    "        fdf_under_365 = data[['Implied Probability <2.5 365','under_2.5_goals','binned <2.5 365','payout_under_2.5_365']]\n",
    "        fdf_under_pinacle = data[['Implied Probability <2.5 pinacle','under_2.5_goals','binned <2.5 pinacle','payout_under_2.5_pinacle']]\n",
    "        fdf_over = data[['Implied Probability >2.5','over_2.5_goals','binned >2.5','payout_over_2.5']]\n",
    "        fdf_over_pinacle = data[['Implied Probability >2.5 pinacle','over_2.5_goals','binned >2.5 pinacle','payout_over_2.5_pinacle']]\n",
    "        fdf_over_365 = data[['Implied Probability >2.5 365','over_2.5_goals','binned >2.5 365','payout_over_2.5_365']]\n",
    "\n",
    "        return fdf, fdf_under, fdf_under_365, fdf_under_pinacle, fdf_over, fdf_over_pinacle, fdf_over_365\n",
    "    else:\n",
    "        fdf = data[['Implied Probability','over_2.5_goals','binned','payout']]\n",
    "        fdf_under = data[['Implied Probability <2.5','under_2.5_goals','binned <2.5','payout_under_2.5']]\n",
    "        fdf_over = data[['Implied Probability >2.5','over_2.5_goals','binned >2.5','payout_over_2.5']]\n",
    "        \n",
    "        return fdf, fdf_under, fdf_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c2669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd232d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = bins.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb62b442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eed45767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b3432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e4596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec1987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e945f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
