{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903a1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaosantos/.pyenv/versions/3.8.12/envs/OnThePitch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35cd5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f1029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(league1, wall=False):\n",
    "    \n",
    "    if wall:\n",
    "        data = pd.DataFrame()\n",
    "        leagues = listdir(f'./../raw_data/')\n",
    "        data = pd.DataFrame()\n",
    "        for league in leagues:\n",
    "            files = listdir(f'./../raw_data/{league}')\n",
    "            for file in files:\n",
    "                df = pd.read_csv((f'./../raw_data/{league}/'+file))\n",
    "                data = pd.concat([data, df])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        files = [file for file in listdir(f'./../raw_data/{league1}')]\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        for file in files:\n",
    "            df = pd.read_csv(f'./../raw_data/{league1}/'+file)\n",
    "            data = pd.concat([data, df])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0176592",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('italy', wall=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "544defe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data, b=20, binned=False):\n",
    "    '''\n",
    "    This function creates all the columns that will be needed to create the analysis \n",
    "    and return the dataframe with all this changes\n",
    "    \n",
    "    b is the number of bins that we want to work with. Our start value for b will be 20.\n",
    "        '''\n",
    "    #------------------------Number of Goals, Over and Under -----------------------------------\n",
    "    \n",
    "    # total number of goals = goals from the home team + goals from visiting team\n",
    "    data['nb_goals']=data['FTHG']+data['FTAG']\n",
    "\n",
    "    # boolean: true or false regarding whether they were more than 2.5 goals\n",
    "    data['over_2.5_goals']=data['nb_goals']>2.5\n",
    "\n",
    "    # boolean: true or false regarding whether they were less than 2.5 goals\n",
    "    data['under_2.5_goals']=data['nb_goals']<2.5\n",
    "    \n",
    "    #-----------------------------Payout Opening ----------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average OPENING odds\n",
    "    data['payout_avg_under_2.5'] = data['under_2.5_goals']*data['Avg<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average OPENING odds\n",
    "    data['payout_avg_over_2.5'] = data['over_2.5_goals']*data['Avg>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE specifically\n",
    "    data['payout_under_2.5_pinacle'] = data['under_2.5_goals']*data['P<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE specifically\n",
    "    data['payout_over_2.5_pinacle'] = data['over_2.5_goals']*data['P>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 specifically\n",
    "    data['payout_under_2.5_365'] = data['under_2.5_goals']*data['B365<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 specifically\n",
    "    data['payout_over_2.5_365'] = data['over_2.5_goals']*data['B365>2.5']\n",
    "    \n",
    "    #------------------------------Payout Closing --------------------------------------------\n",
    "    \n",
    "    # payout under 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_under_closing_2.5'] = data['under_2.5_goals']*data['AvgC<2.5']\n",
    "\n",
    "    # payout over 2.5 for Average CLOSING odds\n",
    "    data['payout_avg_over_closing_2.5'] = data['over_2.5_goals']*data['AvgC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for PINACLE closing ddds specifically\n",
    "    data['payout_under_2.5_pinacle_closing'] = data['under_2.5_goals']*data['PC<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for PINACLE closing odds specifically\n",
    "    data['payout_over_2.5_pinacle_closing'] = data['over_2.5_goals']*data['PC>2.5']\n",
    "\n",
    "    #payout UNDER 2.5 for 365 closing odds specifically\n",
    "    data['payout_under_2.5_365_closing'] = data['under_2.5_goals']*data['B365C<2.5']\n",
    "\n",
    "    #payout OVER 2.5 for 365 closing odds specifically\n",
    "    data['payout_over_2.5_365_closing'] = data['over_2.5_goals']*data['B365C>2.5']\n",
    "    \n",
    "    #-------------------------- Implied Probability Opening ----------------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability <2.5 avg']=1/data['Avg<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for for overall market opening odds (Avg) \n",
    "    data['Implied Probability >2.5 avg']=1/data['Avg>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE\n",
    "    data['Implied Probability <2.5 pinacle']=1/data['P<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE\n",
    "    data['Implied Probability >2.5 pinacle']=1/data['P>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365\n",
    "    data['Implied Probability <2.5 365']=1/data['B365<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365\n",
    "    data['Implied Probability >2.5 365']=1/data['B365>2.5']*100\n",
    "    \n",
    "    #------------------------- Implied Probability Closing -----------------------------------\n",
    "    \n",
    "    #Implied Probability UNDER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability <2.5 avg closing']=1/data['AvgC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for overall market closing odds (AvgC)\n",
    "    data['Implied Probability >2.5 avg closing']=1/data['AvgC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability <2.5 pinacle closing']=1/data['PC<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for PINACLE closing odds\n",
    "    data['Implied Probability >2.5 pinacle closing']=1/data['PC>2.5']*100\n",
    "\n",
    "    #Implied Probability UNDER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability <2.5 365 closing']=1/data['B365C<2.5']*100\n",
    "\n",
    "    #Implied Probability OVER 2.5 goals for 365 closing odds\n",
    "    data['Implied Probability >2.5 365 closing']=1/data['B365C>2.5']*100\n",
    "    \n",
    "    #---------------------------- Binning IP Opening -------------------------------------\n",
    "\n",
    "    b=b\n",
    "    bins = np.arange(0, 101, int(100/b))\n",
    "    bins = bins.tolist()\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned <2.5 avg'] = pd.cut(data['Implied Probability <2.5 avg'], bins)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned >2.5 avg'] = pd.cut(data['Implied Probability >2.5 avg'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned <2.5 pinacle'] = pd.cut(data['Implied Probability <2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned >2.5 pinacle'] = pd.cut(data['Implied Probability >2.5 pinacle'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned <2.5 365'] = pd.cut(data['Implied Probability <2.5 365'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned >2.5 365'] = pd.cut(data['Implied Probability >2.5 365'], bins)\n",
    "    \n",
    "    #----------------------------- Binning IP Closing ------------------------------------------------\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market closing odds\n",
    "    data['binned <2.5 avg closing'] = pd.cut(data['Implied Probability <2.5 avg closing'], bins)\n",
    "\n",
    "    #Binning OVER 2.5 Average Market closing odds\n",
    "    data['binned >2.5 avg closing'] = pd.cut(data['Implied Probability >2.5 avg closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle closing odds\n",
    "    data['binned <2.5 pinacle closing'] = pd.cut(data['Implied Probability <2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle CLOSING odds\n",
    "    data['binned >2.5 pinacle closing'] = pd.cut(data['Implied Probability >2.5 pinacle closing'], bins)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 CLOSING odds\n",
    "    data['binned <2.5 365 closing'] = pd.cut(data['Implied Probability <2.5 365 closing'], bins)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 CLOSING odds\n",
    "    data['binned >2.5 365 closing'] = pd.cut(data['Implied Probability >2.5 365 closing'], bins)\n",
    "    \n",
    "    #---------------------------- Binning Odds Opening ----------------------------------------------------\n",
    "    \n",
    "    bins2 = [1, 1.5, 2, 3, 99999]\n",
    "\n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg'] = pd.cut(data['Avg<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg'] = pd.cut(data['Avg>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle'] = pd.cut(data['P<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle'] = pd.cut(data['P>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365'] = pd.cut(data['B365<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365'] = pd.cut(data['B365>2.5'], bins2)\n",
    "    \n",
    "    #----------------------------- Binning Odds Closing ----------------------------------------------------------\n",
    "    \n",
    "    #Binning UNDER 2.5 Average Market opening odds\n",
    "    data['binned odds <2.5 avg closing'] = pd.cut(data['AvgC<2.5'], bins2)\n",
    "\n",
    "    #Binning Over 2.5 Average Market opening odds\n",
    "    data['binned odds >2.5 avg closing'] = pd.cut(data['AvgC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 Pinnacle opening odds\n",
    "    data['binned odds <2.5 pinacle closing'] = pd.cut(data['PC<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 Pinnacle\n",
    "    data['binned odds >2.5 pinacle closing'] = pd.cut(data['PC>2.5'], bins2)\n",
    "\n",
    "    #Binned UNDER 2.5 bet365 OPENING odds\n",
    "    data['binned odds <2.5 365 closing'] = pd.cut(data['B365C<2.5'], bins2)\n",
    "\n",
    "    #Binned OVER 2.5 bet365 OPENING odds\n",
    "    data['binned odds >2.5 365 closing'] = pd.cut(data['B365C>2.5'], bins2)\n",
    "    \n",
    "    \n",
    "    #----------------------------- Other Features from D3 ------------------------------------------------------\n",
    "    \n",
    "    data['Pin_pays_better_under_boolean'] = data['PC<2.5'] > data['AvgC<2.5']\n",
    "    data['Pin_pays_better_under_difference'] = data['PC<2.5'] / data['AvgC<2.5']\n",
    "    data['%vig_p'] = (1 - (1 / (1/data['PC>2.5'] + 1/data['PC<2.5'])))*100\n",
    "    data['%vig_avg'] = (1 - (1 / (1/data['AvgC>2.5'] + 1/data['AvgC<2.5'])))*100\n",
    "    data['PC<2.5_P_boolean'] = data['PC<2.5'] < data['P<2.5']\n",
    "    data['PC<2.5_P_relative_diff'] = data['PC<2.5'] / data['P<2.5']\n",
    "    \n",
    "    #----------------------- Odds and probability of the home team scoring under 2.5 -------------------------------\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    \n",
    "    lst1 = []\n",
    "    lst2 = []\n",
    "    for i, team in enumerate(data['HomeTeam']):\n",
    "        date = data['Date'].iloc[i]\n",
    "        total = len(data[(data['HomeTeam'] == team) & (data['Date'] < date)])\n",
    "        n_under_home = data[(data['HomeTeam'] == team) & (data['Date'] < date)]['under_2.5_goals'].value_counts()\n",
    "        try:\n",
    "            lst1.append(1/(n_under_home[1]/total))\n",
    "            lst2.append(n_under_home[1]/total)\n",
    "        except:\n",
    "            lst1.append(np.nan)\n",
    "            lst2.append(np.nan)\n",
    "\n",
    "    data['odds_home_under'] = lst1\n",
    "    data['prob_home_under'] = lst2\n",
    "    \n",
    "    \n",
    "    \n",
    "    #----------------------- Odds and probability of the away team scoring under 2.5 -------------------------------\n",
    "    \n",
    "    lst3 = []\n",
    "    lst4  = []\n",
    "    for i, team in enumerate(data['AwayTeam']):\n",
    "        date = data['Date'].iloc[i]\n",
    "        total2 = len(data[(data['AwayTeam'] == team) & (data['Date'] < date)])\n",
    "        n_under_away2 = data[(data['AwayTeam'] == team) & (data['Date'] < date)]['under_2.5_goals'].value_counts()\n",
    "        try:\n",
    "            lst3.append(1/(n_under_away2[1] / total2))\n",
    "            lst4.append(n_under_away2[1] / total2)\n",
    "        except:\n",
    "            lst3.append(np.nan)\n",
    "            lst4.append(np.nan)\n",
    "\n",
    "    data['odds_away_under'] = lst3\n",
    "    data['prob_away_under'] = lst4\n",
    "    \n",
    "        #----------------------- Odds and probability of the home team scoring over 2.5 -------------------------------\n",
    "    \n",
    "    lst5 = []\n",
    "    lst6 = []\n",
    "    for i, team in enumerate(data['HomeTeam']):\n",
    "        date = data['Date'].iloc[i]\n",
    "        total = len(data[(data['HomeTeam'] == team) & (data['Date'] < date)])\n",
    "        n_under_home = data[(data['HomeTeam'] == team) & (data['Date'] < date)]['over_2.5_goals'].value_counts()\n",
    "        try:\n",
    "            lst5.append(1/(n_under_home[1]/total))\n",
    "            lst6.append(n_under_home[1]/total)\n",
    "        except:\n",
    "            lst5.append(np.nan)\n",
    "            lst6.append(np.nan)\n",
    "\n",
    "    data['odds_home_over'] = lst5\n",
    "    data['prob_home_over'] = lst6\n",
    "    \n",
    "     #----------------------- Odds and probability of the away team scoring over 2.5 -------------------------------\n",
    "    \n",
    "    lst7 = []\n",
    "    lst8  = []\n",
    "    for i, team in enumerate(data['AwayTeam']):\n",
    "        date = data['Date'].iloc[i]\n",
    "        total2 = len(data[(data['AwayTeam'] == team) & (data['Date'] < date)])\n",
    "        n_under_away2 = data[(data['AwayTeam'] == team) & (data['Date'] < date)]['over_2.5_goals'].value_counts()\n",
    "        try:\n",
    "            lst7.append(1/(n_under_away2[1] / total2))\n",
    "            lst8.append(n_under_away2[1] / total2)\n",
    "        except:\n",
    "            lst7.append(np.nan)\n",
    "            lst8.append(np.nan)\n",
    "\n",
    "    data['odds_away_over'] = lst7\n",
    "    data['prob_away_over'] = lst8\n",
    "    \n",
    "    # -------------------- binning the odds and probability of the home and away teams under 2.5 ----------------------\n",
    "    if binned:\n",
    "\n",
    "        #------- Probability -------\n",
    "\n",
    "        #binning the probability of the home team to have a game of less than 2.5 score\n",
    "        data['binned prob_home_under'] = pd.cut(data['prob_home_under']*100, bins)\n",
    "\n",
    "        #binning the probability of the away team to have a game of less than 2.5 score\n",
    "        data['binned prob_away_under'] = pd.cut(data['prob_away_under']*100, bins)\n",
    "\n",
    "        #--------- Odds ------------\n",
    "        binodds = [1, 1.25, 1.42, 1.5, 1.6, 1.8, 2, 2.2, 2.5, 2.8, 3.5, 4, 100]\n",
    "\n",
    "        #binning the odds of the away team to have a game of less than 2.5 score\n",
    "        data['binned odds_away_under'] = pd.cut(data['odds_away_under'], binodds)\n",
    "\n",
    "        #binning the odds of the home team to have a game of less than 2.5 score\n",
    "        data['binned odds_home_under'] = pd.cut(data['odds_away_under'], binodds)\n",
    "\n",
    "\n",
    "        # -------------------- binning the odds and probability of the home and away teams over 2.5 ----------------------\n",
    "\n",
    "        #------- Probability -------\n",
    "\n",
    "        #binning the probability of the home team to have a game of less than 2.5 score\n",
    "        data['binned prob_home_over'] = pd.cut(data['prob_home_over']*100, bins)\n",
    "\n",
    "        #binning the probability of the away team to have a game of less than 2.5 score\n",
    "        data['binned prob_away_over'] = pd.cut(data['prob_away_over']*100, bins)\n",
    "\n",
    "        #--------- Odds ------------\n",
    "        binodds = [1, 1.25, 1.42, 1.5, 1.6, 1.8, 2, 2.2, 2.5, 2.8, 3.5, 4, 100]\n",
    "\n",
    "        #binning the odds of the away team to have a game of less than 2.5 score\n",
    "        data['binned odds_away_over'] = pd.cut(data['odds_away_over'], binodds)\n",
    "\n",
    "        #binning the odds of the home team to have a game of less than 2.5 score\n",
    "        data['binned odds_home_over'] = pd.cut(data['odds_away_over'], binodds)\n",
    "\n",
    "\n",
    "    #-------------------------- Creating the prob and odds of the game -----------------------------------------------\n",
    "    \n",
    "    #---------------- Under --------------\n",
    "    '''the mean between the probability of the home team to have a score of under 2.5 and the probability \n",
    "    of the away team to do the same'''\n",
    "    \n",
    "    data['odds_game_under'] = (data['odds_away_under'] +  data['odds_home_under']) / 2\n",
    "    data['prob_game_under'] = (data['prob_away_under'] + data['prob_home_under']) / 2\n",
    "    \n",
    "    #---------------- Over -------------\n",
    "\n",
    "    '''the mean between the probability of the home team to have a score of over 2.5 and the probability \n",
    "    of the away team to do the same'''\n",
    "    \n",
    "    data['odds_game_over'] = (data['odds_away_over'] +  data['odds_home_over']) / 2\n",
    "    data['prob_game_over'] = (data['prob_away_over'] + data['prob_home_over']) / 2\n",
    "    \n",
    "    #-------------------------- OneHotEncoding the binned probabilities columns ------------------------------------------\n",
    "    \n",
    "    if binned:\n",
    "        if b == 5:\n",
    "            #-------------------- Under -----------------------\n",
    "            data = data[~data['binned prob_home_under'].isna()]\n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe.fit(data[['binned prob_home_under']])\n",
    "            bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "            data[\"0, 20\"], data[\"20, 40\"], data[\"40, 60\"], data[\"60, 80\"], data[\"80, 100\"] = bins_encoded.T\n",
    "\n",
    "            #-------------------- Over -----------------------\n",
    "            data = data[~data['binned prob_home_over'].isna()]\n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe.fit(data[['binned prob_home_over']])\n",
    "            bins_encoded = ohe.transform(data[['binned prob_home_over']])\n",
    "            data[\"0, 20\"], data[\"20, 40\"], data[\"40, 60\"], data[\"60, 80\"], data[\"80, 100\"] = bins_encoded.T\n",
    "\n",
    "        if b == 10:\n",
    "            #-------------------- Under -----------------------\n",
    "            data = data[~data['binned prob_home_under'].isna()]\n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe.fit(data[['binned prob_home_under']])\n",
    "            bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "            data[\"0, 10\"], data[\"10, 20\"], data[\"20, 30\"], data[\"30, 40\"], data[\"40, 50\"], data[\"50, 60\"], \\\n",
    "            data[\"60, 70\"], data[\"70, 80\"], data[\"80, 90\"], data[\"90, 100\"] = bins_encoded.T\n",
    "\n",
    "            #-------------------- Over -----------------------\n",
    "            data = data[~data['binned prob_home_over'].isna()]\n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe.fit(data[['binned prob_home_over']])\n",
    "            bins_encoded = ohe.transform(data[['binned prob_home_over']])\n",
    "            data[\"0, 10\"], data[\"10, 20\"], data[\"20, 30\"], data[\"30, 40\"], data[\"40, 50\"], data[\"50, 60\"], \\\n",
    "            data[\"60, 70\"], data[\"70, 80\"], data[\"80, 90\"], data[\"90, 100\"] = bins_encoded.T\n",
    "\n",
    "        if b == 20:\n",
    "            #-------------------- Under -----------------------\n",
    "            data = data[~data['binned prob_home_under'].isna()]\n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe.fit(data[['binned prob_home_under']])\n",
    "            bins_encoded = ohe.transform(data[['binned prob_home_under']])\n",
    "            data[\"0, 5\"], data[\"5, 10\"], data[\"10, 15\"], data[\"15, 20\"], data[\"20, 25\"], data[\"25, 30\"], \\\n",
    "            data[\"30, 35\"], data[\"35, 40\"], data[\"40, 45\"], data[\"45, 50\"], data[\"50, 55\"], data[\"55, 60\"], \\\n",
    "            data[\"60, 65\"], data[\"65, 70\"], data[\"70, 75\"], data[\"75, 80\"], data[\"80, 85\"], data[\"85, 90\"], \\\n",
    "            data[\"90, 95\"], data[\"95, 100\"]= bins_encoded.T\n",
    "\n",
    "            #-------------------- Over -----------------------\n",
    "            data = data[~data['binned prob_home_over'].isna()]\n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe.fit(data[['binned prob_home_over']])\n",
    "            bins_encoded = ohe.transform(data[['binned prob_home_over']])\n",
    "            data[\"0, 5\"], data[\"5, 10\"], data[\"10, 15\"], data[\"15, 20\"], data[\"20, 25\"], data[\"25, 30\"], \\\n",
    "            data[\"30, 35\"], data[\"35, 40\"], data[\"40, 45\"], data[\"45, 50\"], data[\"50, 55\"], data[\"55, 60\"], \\\n",
    "            data[\"60, 65\"], data[\"65, 70\"], data[\"70, 75\"], data[\"75, 80\"], data[\"80, 85\"], data[\"85, 90\"], \\\n",
    "            data[\"90, 95\"], data[\"95, 100\"]= bins_encoded.T\n",
    "\n",
    "    #-------------------------- OneHotEncoding the binned odds columns ------------------------------------------\n",
    "    \n",
    "        #-------------------- Under -----------------------                                       \n",
    "        data = data[~data['binned odds_away_under'].isna()]\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe.fit(data[['binned odds_away_under']])\n",
    "        bins_encoded = ohe.transform(data[['binned odds_away_under']])\n",
    "        data[\"1.0, 1.25\"], data[\"1.25, 1.42\"], data[\"1.42, 1.5\"], data[\"1.5, 1.6\"],\\\n",
    "        data[\"1.6, 1.8\"], data[\"1.6, 1.8\"], data[\"1.8, 2.0\"], data[\"2.0, 2.2\"], \\\n",
    "        data[\"2.2, 2.5\"], data[\"2.5, 2.8\"], data[\"2.8, 3.5\"], data[\"3.5, 4.0\"] = bins_encoded.T\n",
    "\n",
    "        #-------------------- Over -----------------------\n",
    "        data = data[~data['binned odds_away_over'].isna()]\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe.fit(data[['binned odds_away_over']])\n",
    "        bins_encoded = ohe.transform(data[['binned odds_away_over']])\n",
    "        data[\"1.0, 1.25\"], data[\"1.25, 1.42\"], data[\"1.42, 1.5\"], data[\"1.5, 1.6\"],\\\n",
    "        data[\"1.6, 1.8\"], data[\"1.6, 1.8\"], data[\"1.8, 2.0\"], data[\"2.0, 2.2\"], \\\n",
    "        data[\"2.2, 2.5\"], data[\"2.5, 2.8\"], data[\"2.8, 3.5\"], data[\"3.5, 4.0\"] = bins_encoded.T\n",
    "    \n",
    "    #------------------------------------ Cleaning the data ---------------------------------------------------------\n",
    "    \n",
    "    #data = data.dropna(subset=['HomeTeam', 'AwayTeam'], how='any')\n",
    "    data = data[~data['HomeTeam'].isna()]\n",
    "    data = data[~data['AwayTeam'].isna()]\n",
    "    data = data[~data['PC>2.5'].isna()]\n",
    "    data.drop(columns=['Referee', 'Unnamed: 105'], inplace=True)\n",
    "    #data.dropna()\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065c2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feature_engineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce98d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20905, 175)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be61049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the features that we will use in the model to predict the under\n",
    "X = data[['odds_home_over','odds_away_over','odds_home_under','odds_away_under', 'odds_game_over', 'odds_game_under', 'PC<2.5', 'AvgC<2.5','PC>2.5', 'AvgC>2.5']]\n",
    "y = data['payout_under_2.5_pinacle_closing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2c92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scalling our features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab052b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7d8384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model in the training data\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96d8423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation score: -0.09\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, X_train, y_train,cv=10)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8955d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.3606561653213647\n"
     ]
    }
   ],
   "source": [
    "score = model.score(X_train, y_train)  \n",
    "print(\"Training score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef74b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0122361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
